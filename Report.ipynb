{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/BannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "\"C:\\Projects\\Value-based-methods\\p1_navigation\\Banana_Windows_x86_64\\Banana_Windows_x86_64\\Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $s$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "<BR>\n",
    "\n",
    "The neural network will be a fully connected MLP with one input for each state variable, at least one hidden layer, and one output for each pair $(z_i, a)$.  See the implentation section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47142b86",
   "metadata": {},
   "source": [
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameterized model.  Like in [1], the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\omega}, \\omega \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\omega$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [3].\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884e3f",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "`state_dimension`: Dimension of the observable state space<br>\n",
    "`num_actions`: Number of actions available to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5746ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentHyperparameters():\n",
    "    def __init__(self,state_dimension=37,num_actions=4):\n",
    "        self.state_dimension = state_dimension\n",
    "        self.num_actions = num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43249fb8",
   "metadata": {},
   "source": [
    "#### Parameterized Model\n",
    "`num_hidden_layers`: How many hidden layers are included in the neural network model<br>\n",
    "`hidden_layer_size`: How many neurons are included in each hidden layer<br>\n",
    "`noise_init`: Initial standard deviation for all noise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f239be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters():\n",
    "    def __init__(self,num_hidden_layers=2,hidden_layer_size=400,noise_init=0.03):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.noise_init = noise_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c93654",
   "metadata": {},
   "source": [
    "#### Reward Distribution\n",
    "`v_min`: Lower limit clipping value for *n-step* return<br>\n",
    "`v_max`: Upper limit clipping value for *n-step* return<br>\n",
    "`gamma`: Discount factor for reward calculation.  An expected reward n steps in the future is discounted by $\\gamma^n$<br>\n",
    "`n_step_order`: How many environment steps from initial state are considered in constructing action value distribution<br>\n",
    "`n_atoms`: Number of discretization bins for representing possible value distribution returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e717b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionHyperparameters():\n",
    "    def __init__(self,v_min=-6,v_max=6,gamma=0.98,n_step_order=25,n_atoms=51):\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.gamma = gamma\n",
    "        self.n_step_order = n_step_order\n",
    "        self.n_atoms = n_atoms\n",
    "        self.z_i = \\\n",
    "            torch.tensor([(v_min + (i * ((v_max-v_min) / (n_atoms - 1)))) for i in range(n_atoms)])\n",
    "        self.z_i = torch.unsqueeze(self.z_i,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782dd24",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "`buffer_life`: Buffer will be reset after this many sample() calls<br>\n",
    "`omega`: Loss influence factor for transition event selection probabilities<br>\n",
    "`beta`: Importance sampling correction coefficient.  See [3](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferHyperparameters():\n",
    "    def __init__(self,buffer_life=9000, omega=0.6, beta=1.0):\n",
    "        self.buffer_life = buffer_life\n",
    "        self.omega = omega\n",
    "        self.beta = beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dff3c",
   "metadata": {},
   "source": [
    "#### Optimization Behavior\n",
    "`lr`: Learning rate for parameterized model optimizer<br>\n",
    "`learn_interval`: Number of environment exploration steps between each learning phase<br>\n",
    "`learn_steps`: Number of learning steps (in batches) per learning phase<br>\n",
    "`batch_size`: Number of n-step transition events to process during each learning phase <br>\n",
    "`purge_replay`: Boolean, if True, replay buffer will be reset after each learning phase <br>\n",
    "`target_alpha`: Soft update factor used so that target reward distribution lags policy reward distribution.  See [this explanation](https://deeplizard.com/learn/video/xVkPh9E9GfE) for why this is necessary.  The following update will be applied once per batch:<br><br>\n",
    "$$\\theta_{target} = (1-\\alpha)\\theta_{target} + \\alpha\\theta_{policy}$$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb02b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHyperparameters():\n",
    "    def __init__(self,\n",
    "                 lr=0.0000125,\n",
    "                 learn_interval=(5*300),\n",
    "                 learn_steps=(600),\n",
    "                 batch_size=16,\n",
    "                 purge_replay=False,\n",
    "                 target_alpha=0.25):\n",
    "        self.lr = lr\n",
    "        self.learn_interval = learn_interval\n",
    "        self.learn_steps = learn_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.purge_replay = purge_replay\n",
    "        self.target_alpha = target_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484595f5",
   "metadata": {},
   "source": [
    "Like in [3](https://arxiv.org/abs/1511.05952), a sum tree structure is used for efficient weighted sampling over a large number of items.  This implementation is heavily based on that in reference [4](http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/), but with some organizational changes to accomodate dynamic resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that represents an experience in the experience buffer and/or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def remove(self):\n",
    "        if self.parent is not None:  # remove only has effect if there is a parent\n",
    "            if self.parent.left_child is self:\n",
    "                sibling = self.parent.right_child\n",
    "            else:\n",
    "                sibling = self.parent.left_child\n",
    "            # Clone data to parent from sibling and update references\n",
    "            if sibling is None:\n",
    "                print(f'Removing:{self.data} |||| {self.p_i}')\n",
    "                print(f'Parent:{self.parent.data} |||| {self.parent.p_i}')\n",
    "                print(f'Sibling:{sibling.data} |||| {sibling.p_i}')\n",
    "            self.parent.data = sibling.data\n",
    "            self.parent.left_child = sibling.left_child\n",
    "            self.parent.right_child = sibling.right_child\n",
    "            if (self.parent.left_child is not None):\n",
    "                self.parent.left_child.parent = self.parent\n",
    "            if (self.parent.right_child is not None):\n",
    "                self.parent.right_child.parent = self.parent\n",
    "            self.parent.update_p(sibling.p_i - self.parent.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "            \n",
    "    # This is used to identify low sampling probability nodes for removal\n",
    "    def minimal_node(self):\n",
    "        if self.data is not None:\n",
    "            return self\n",
    "        else:\n",
    "            min_left = self.left_child.minimal_node()\n",
    "            min_right = self.right_child.minimal_node()\n",
    "            return min_left if min_left.p_i < min_right.p_i else min_right  \n",
    "    \n",
    "    # String representation of tree\n",
    "    def tree_str(self,lvl=0):\n",
    "        lvl_str= '--> '*lvl\n",
    "        left_str = self.left_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        right_str = self.right_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        par_str = self.parent.p_i if self.parent is not None else 'nothing'\n",
    "        my_str = f'{lvl_str}[{self.data}|{self.p_i}] under [{par_str}]\\n'\n",
    "        return f'{my_str}{left_str}{right_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95526535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized replay buffer definition\n",
    "\n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "\n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,replay_buffer_hyperparameters=ReplayBufferHyperparameters()):\n",
    "        self.buffer_life = replay_buffer_hyperparameters.buffer_life\n",
    "        self.omega = replay_buffer_hyperparameters.omega\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = replay_buffer_hyperparameters.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.omega)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.beta,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buffer_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('Flushed replay buffer!')\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "068dc3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Counts\n",
      "one: 0.9414225941422594\n",
      "two: 1.7656903765690377\n",
      "three: 2.702928870292887\n",
      "four: 1.0\n",
      "five: 1.8828451882845187\n",
      "six: 2.912133891213389\n",
      "seven: 0.9205020920502092\n",
      "eight: 1.8284518828451883\n",
      "nine: 2.782426778242678\n"
     ]
    }
   ],
   "source": [
    "# Unit test for PrioritizedReplayBuffer\n",
    "test_rep_hyp = ReplayBufferHyperparameters(buffer_life = 2000, omega = 1)\n",
    "test_rep_buffer = PrioritizedReplayBuffer(test_rep_hyp)\n",
    "test_experiences = ['one','two','three','four','five','six','seven','eight','nine']\n",
    "test_weights = [0.99,2,3,1,2,3,1,2,3]\n",
    "test_batch_size = 4\n",
    "test_batches = 1000\n",
    "for (e,wt) in zip(test_experiences,test_weights):\n",
    "    test_rep_buffer.add_experience(e,wt)\n",
    "test_samples = []\n",
    "for i in range(test_batches):\n",
    "    test_samples += [sample.data for sample in test_rep_buffer.sample(test_batch_size)[0]]\n",
    "print(\"Normalized Counts\")\n",
    "for exp in test_experiences:\n",
    "    print(f'{exp}: {test_samples.count(exp) / test_samples.count(\"four\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df9650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic function\n",
    "def tensor_check(input,desc):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}:')\n",
    "        print(input)\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}:')\n",
    "        print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3bb112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in A/B:\n",
      "tensor([1.5000,    nan, 1.0000])\n",
      "Inf in C/B:\n",
      "tensor([0.5000,    inf, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# tensor_check unit test\n",
    "A = torch.tensor([3,0,1])\n",
    "B = torch.tensor([2,0,1])\n",
    "C = torch.tensor([1,1,1])\n",
    "tensor_check(torch.div(A,B),\"A/B\")\n",
    "tensor_check(torch.div(C,B),\"C/B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e6e6",
   "metadata": {},
   "source": [
    "The linear layers have noisy bias and weight components, implemented with factored gaussian noise according to [5](https://arxiv.org/abs/1706.10295).  The standard deviation of each noise component is subject to optimization (learning) by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_init=0.05):\n",
    "        super(NoisyLinear,self).__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            nn.Parameter(noise_init*torch.ones(in_dim,out_dim,dtype=torch.float32))\n",
    "        self.noisy_bias = nn.Parameter(noise_init*torch.ones(1,out_dim,dtype=torch.float32))\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [5]\n",
    "        tensor_check(x,'Input to noise_transform')\n",
    "        x = torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "        tensor_check(x,'Output of noise_transform')\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized gaussian noise, clamping to +/- 5 sigma\n",
    "        in_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((self.noisy_weights.size(dim=0),1),device=x.device),-5.0,5.0))\n",
    "        out_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((1,self.noisy_weights.size(dim=1)),device=x.device),-5.0,5.0))\n",
    "        \n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "    \n",
    "        x = x.float()\n",
    "        return self.deterministic_linear(x) + torch.matmul(x,weight_noise) + bias_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "319ce3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:torch.Size([5, 10])\n",
      "Output size:torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Unit test for NoisyLinear\n",
    "test_noisy_layer = NoisyLinear(10,4)\n",
    "test_input_data = torch.ones((5,10))\n",
    "test_output = test_noisy_layer(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f76476cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterized Model Implementation\n",
    "class ParameterizedModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 environment_hyperparameters = EnvironmentHyperparameters(),\n",
    "                 model_hyperparameters = ModelHyperparameters(),\n",
    "                 distribution_hyperparameters = DistributionHyperparameters()\n",
    "                ):\n",
    "        super(ParameterizedModel,self).__init__()\n",
    "        in_dim = environment_hyperparameters.state_dimension\n",
    "        out_dim = environment_hyperparameters.num_actions \\\n",
    "                  * distribution_hyperparameters.n_atoms\n",
    "        hidden_size = model_hyperparameters.hidden_layer_size\n",
    "        num_hidden_layers = model_hyperparameters.num_hidden_layers\n",
    "        noise_init = model_hyperparameters.noise_init\n",
    "        \n",
    "        self.distribution_hyperparameters = distribution_hyperparameters\n",
    "        self.environment_hyperparameters = environment_hyperparameters\n",
    "        self.model_hyperparameters = model_hyperparameters\n",
    "        self.num_actions = environment_hyperparameters.num_actions\n",
    "        self.n_atoms = distribution_hyperparameters.n_atoms\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(NoisyLinear(in_dim,hidden_size,noise_init))\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        for i in range(num_hidden_layers-1):\n",
    "            layers.append(NoisyLinear(hidden_size,hidden_size,noise_init))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        layers.append(NoisyLinear(hidden_size,out_dim,noise_init))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.reg_layers(x)\n",
    "        \n",
    "        # See reference [1], softmax must be calculated individually on the\n",
    "        # atoms for each action since each action is a probability distribution\n",
    "        y = torch.tensor([],requires_grad=True,device=x.device)\n",
    "        for i in range(self.num_actions):\n",
    "            y = torch.hstack((y,F.softmax(x[:,(i*self.n_atoms):((i+1)*self.n_atoms)],dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b4908a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:torch.Size([5, 11])\n",
      "Output size:torch.Size([5, 21])\n"
     ]
    }
   ],
   "source": [
    "# Unit test for ParameterizedModel\n",
    "test_env_hyp = EnvironmentHyperparameters(11,3)\n",
    "test_mdl_hyp = ModelHyperparameters(3,30)\n",
    "test_dst_hyp = DistributionHyperparameters(n_atoms = 7)\n",
    "test_mdl = ParameterizedModel(test_env_hyp, test_mdl_hyp, test_dst_hyp)\n",
    "test_input_data = torch.randn(5,11)\n",
    "test_output = test_mdl(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca8356",
   "metadata": {},
   "source": [
    "See algorithm 1 in reference [2](https://arxiv.org/abs/1707.06887).  Instead of cross-entropy loss on the last line, instead the computed loss is the KL divergence between the current distribution and the target distribution, as in [1](https://arxiv.org/abs/1710.02298)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_Update(policy_net, target_net, experiences, device):\n",
    "    \n",
    "    distribution_hyperparameters = policy_net.distribution_hyperparameters\n",
    "    \n",
    "    # Convert experiences to tensors, short alias \n",
    "    # hyperparameters since somewhat intense here\n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    actions = torch.tensor([e.data.action for e in experiences[0]]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences[0]]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.tensor(np.array([e.data.last_state for e in experiences[0]])).to(device)\n",
    "    \n",
    "    p_i_total = experiences[1][2]\n",
    "    # Normalized p_i see eqn (1) of reference [3]\n",
    "    p_i = torch.tensor([(e.p_i/p_i_total) for e in experiences[0]]).to(device) \n",
    "    \n",
    "    buf_N = torch.tensor(experiences[1][0]).to(device)\n",
    "    beta = torch.tensor(experiences[1][1]).to(device)\n",
    "    gamma = torch.tensor([distribution_hyperparameters.gamma]).to(device)\n",
    "    n_step_order = torch.tensor([distribution_hyperparameters.n_step_order]).to(device)\n",
    "    v_min = torch.tensor([distribution_hyperparameters.v_min]).to(device)\n",
    "    v_max = torch.tensor([distribution_hyperparameters.v_max]).to(device)\n",
    "    z_i = distribution_hyperparameters.z_i.to(device)\n",
    "    n_atoms = policy_net.n_atoms\n",
    "    delta_z = (v_max - v_min) / (n_atoms - 1)\n",
    "    num_exp = len(init_states)\n",
    "    num_act = policy_net.num_actions\n",
    "    \n",
    "    # Input checks\n",
    "    tensor_check(p_i,'p_i')\n",
    "    \n",
    "    # Get d_t\n",
    "    d_t_all_actions = policy_net(init_states)\n",
    "    tensor_check(d_t_all_actions,'d_t_all_actions')\n",
    "    d_t = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t = torch.vstack((d_t,d_t_all_actions[i,(actions[i]*n_atoms):((actions[i]+1)*n_atoms)]))\n",
    "    tensor_check(d_t,'d_t')\n",
    "    \n",
    "    # Get d_t_prime\n",
    "    d_t_prime_all_actions = target_net(final_states)\n",
    "    mean_action_values = torch.empty((num_exp,0),requires_grad=True).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = \\\n",
    "            torch.hstack((mean_action_values,\n",
    "                torch.matmul(d_t_prime_all_actions[:,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    targ_act = torch.argmax(mean_action_values,dim=1)\n",
    "    d_t_prime = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t_prime = torch.vstack( \\\n",
    "            (d_t_prime,\n",
    "             d_t_prime_all_actions[i,(targ_act[i]*n_atoms):((targ_act[i]+1)*n_atoms)]))\n",
    "    tensor_check(d_t_prime,'d_t_prime')\n",
    "        \n",
    "    # Compute target distribution m_i.  See algorithm 1 of reference [2]\n",
    "    m_i = torch.zeros((num_exp,n_atoms)).to(device)\n",
    "    disc = torch.pow(gamma,n_step_order).to(device)\n",
    "    Tz_j = torch.clamp(torch.add(rewards,torch.mul(disc,d_t_prime)), v_min, v_max)\n",
    "    b_j = torch.div(torch.sub(Tz_j,v_min),delta_z)\n",
    "    l_bins = torch.floor(b_j).long()\n",
    "    u_bins = torch.ceil(b_j).long()\n",
    "    tensor_check(Tz_j,'Tz_j')\n",
    "    tensor_check(b_j,'b_j')\n",
    "    for j in range(n_atoms):\n",
    "        test_rslt = torch.sub(u_bins[:,j],b_j[:,j])\n",
    "        m_i[torch.arange(num_exp).long(),l_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),l_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(u_bins[:,j],b_j[:,j])))\n",
    "        m_i[torch.arange(num_exp).long(),u_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),u_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(b_j[:,j],l_bins[:,j])))\n",
    "    \n",
    "    # Compute KL divergence between distributions for each experience\n",
    "    # Return loss for all experiences in batch.  We do not reduce to mean\n",
    "    # because the individual losses are needed to update the prioritized\n",
    "    # experience replay buffer.\n",
    "    biased_loss = F.kl_div(d_t.log(),m_i,reduction='none')\n",
    "    tensor_check(biased_loss,'biased_loss')\n",
    "    \n",
    "    # Multiply losses by importance sampling weights to correct for distribution change\n",
    "    # with prioritized sampling.  Clamp p_i because reciprocal function is scary\n",
    "    p_i = torch.clamp(p_i,min=1e-9)\n",
    "    w_i = torch.pow( \\\n",
    "            torch.reciprocal( \\\n",
    "              torch.mul(buf_N,p_i)),beta).unsqueeze(dim=1)\n",
    "    w_i = torch.clamp(w_i,1e-8,1.0)\n",
    "    tensor_check(w_i,'w_i')\n",
    "    return (torch.mul(w_i,biased_loss).clamp(min=0), w_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729af8",
   "metadata": {},
   "source": [
    "#### Training Session Options\n",
    "`max_episodes`: Maximum number of episodes before stopping training run<br>\n",
    "`rewards_buffer`: Pass in empty list or existing list.  Average rewards per episode data is appended to this list<br>\n",
    "`reward_window`: Length of averaging window for reporting rewards, in number of episodes<br>\n",
    "`report_interval`: Interval in episodes for updating `rewards_buffer` and displaying status<br>\n",
    "`solved_threshold`: Average reward over averaging window required to stop training early<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93602f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the episodic scores and running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_window_size=100,starting_scores=None):\n",
    "        self.avg_window_size = avg_window_size\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_window_size\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_window_size:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_window_size + 1)] / self.avg_window_size\n",
    "            \n",
    "    def run_avg(self):\n",
    "        if len(self.scores) < self.avg_window_size:\n",
    "            return None\n",
    "        else:\n",
    "            return self.internal_run_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a07bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First average: 49.5\n",
      "Second average: 149.5\n"
     ]
    }
   ],
   "source": [
    "# Unit test PerformanceLogger\n",
    "test_perf_log = PerformanceLogger()\n",
    "for i in range(100):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'First average: {test_perf_log.run_avg()}')\n",
    "for i in range(100,200):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'Second average: {test_perf_log.run_avg()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c86e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOptions():\n",
    "    def __init__(self,max_episodes=2000,\n",
    "                 rewards_buffer=[],reward_window=100,\n",
    "                 report_interval=5,solved_threshold=13):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.rewards_buffer = rewards_buffer\n",
    "        self.report_interval = report_interval\n",
    "        self.solved_threshold = solved_threshold\n",
    "        self.performance_logger = PerformanceLogger(reward_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab2728d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular buffer for the n-step rewards.  \n",
    "# Could be more efficient but n is only 3 \n",
    "# in methods of rainbow dqn paper and it worked\n",
    "# out for them.\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,n_step_order,gamma=1.0):\n",
    "        self.n_step_order = n_step_order\n",
    "        self.store = deque(maxlen = n_step_order + 1)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == (self.n_step_order + 1)\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbdca606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is copied from the Lunar Lander dqn_agent.py file.\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects action number that has the highest expected return\n",
    "def sel_action(policy_net, state, device):\n",
    "    z_i = policy_net.distribution_hyperparameters.z_i.to(device)\n",
    "    num_act = policy_net.environment_hyperparameters.num_actions\n",
    "    n_atoms = policy_net.distribution_hyperparameters.n_atoms\n",
    "    \n",
    "    d_t = policy_net(torch.tensor(state).unsqueeze(dim=0).to(device))\n",
    "    mean_action_values = torch.tensor([],requires_grad=False).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = torch.hstack( \\\n",
    "                                (mean_action_values,\n",
    "                                 torch.matmul(d_t[0,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    return torch.argmax(mean_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73fa6",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`net`: Parameterized model to use for training run<br>\n",
    "`replay_buffer`: Replay buffer object to use for training run<br>\n",
    "`optimization_hyperparameters`: `OptimizationHyperparameters` object to use for training run<br>\n",
    "`train_options`: `TrainingOptions` object to use for training run\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to the `PerformanceLogger` of the supplied `train_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ee256af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, \n",
    "              env,\n",
    "              opt_hyp=OptimizationHyperparameters(),\n",
    "              replay_buffer=PrioritizedReplayBuffer(),\n",
    "              train_options=TrainingOptions(),\n",
    "              ):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move model to device\n",
    "    policy_net = policy_net.to(device)\n",
    "    \n",
    "    # Aliases for readability only\n",
    "    perf_log = train_options.performance_logger\n",
    "    max_eps = train_options.max_episodes\n",
    "    n_step_order = policy_net.distribution_hyperparameters.n_step_order\n",
    "    gamma = policy_net.distribution_hyperparameters.gamma\n",
    "    omega = replay_buffer.omega\n",
    "    batch_size = opt_hyp.batch_size\n",
    "    learn_steps = opt_hyp.learn_steps\n",
    "    purge_replay = opt_hyp.purge_replay\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = Adam(policy_net.parameters(), lr=opt_hyp.lr, weight_decay=5e-5, eps = 1.5e-4)\n",
    "    \n",
    "    # Generate target network\n",
    "    target_net = ParameterizedModel(policy_net.environment_hyperparameters,\n",
    "                                    policy_net.model_hyperparameters,\n",
    "                                    policy_net.distribution_hyperparameters).to(device)\n",
    "    \n",
    "    # Set initial parameters of target network to zero to avoid initial noise\n",
    "    for param in target_net.parameters():\n",
    "        param.data.fill_(0)\n",
    "    \n",
    "    # Counters\n",
    "    learn_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Get brain info for environment.  That is the term\n",
    "    # Unity ML-Agents for controllers of agents in\n",
    "    # simulation\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    for episode in range(max_eps):\n",
    "        \n",
    "        n_step_buf = MultistepBuffer(n_step_order, gamma)\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            \n",
    "            # Environment interaction\n",
    "            action = sel_action(policy_net, state, device)\n",
    "            env_info = env.step(int(action.cpu()))[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # Process interaction results\n",
    "            score += reward\n",
    "            if not (reward == 0):\n",
    "                print(f'Episode {episode+1}: score = {score}', end='\\r')\n",
    "            n_step_buf.add_experience(Experience(state,action,reward,None))\n",
    "            \n",
    "            # If the multi-step buffer has enough experiences to make\n",
    "            # a full sequence, get the sequence, compute the loss on\n",
    "            # the sequence, add the experience with loss to the replay\n",
    "            # buffer\n",
    "            if n_step_buf.ready():\n",
    "                n_step_exp = n_step_buf.get_n_step_experience()\n",
    "                loss, w_i = Bellman_Update(policy_net, target_net, ([n_step_exp],(1,1,1)), device)\n",
    "                n_step_p_i = loss[0].sum().pow(omega)\n",
    "                tensor_check(n_step_p_i,'n_step_p_i')\n",
    "                replay_buffer.add_experience(n_step_exp.data,n_step_p_i.item())\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "            learn_cntr += 1\n",
    "            if (learn_cntr % opt_hyp.learn_interval) == 0:\n",
    "                learn_cntr = 0\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    opt_perf_log = PerformanceLogger()\n",
    "                    for l_step in range(learn_steps):\n",
    "                        \n",
    "                        # Check that a sample op did not trigger reset\n",
    "                        if len(replay_buffer) < batch_size:\n",
    "                            break\n",
    "                        samp_exp = replay_buffer.sample(batch_size)\n",
    "                    \n",
    "                        # Zero gradients, get Bellman_Update loss, take mean, accumulate\n",
    "                        # gradients backward and step optimizer.\n",
    "                        optimizer.zero_grad()\n",
    "                        loss_vec, w_i = \\\n",
    "                            Bellman_Update(policy_net, target_net, samp_exp, device)\n",
    "                        loss_vec = loss_vec.sum(dim=1)\n",
    "                        loss_avg = torch.div(torch.sum(loss_vec), batch_size)\n",
    "                        loss_avg.backward()\n",
    "                        optimizer.step()\n",
    "                        opt_perf_log.add_score(loss_avg.item())\n",
    "                        \n",
    "                        # Update priorities in replay buffer according to losses\n",
    "                        new_p_i = torch.div(loss_vec,w_i.squeeze(dim=1)).pow(omega)\n",
    "                        tensor_check(new_p_i,'new_p_i')\n",
    "                        for exp_num in range(batch_size):\n",
    "                                samp_exp[0][exp_num].update_p( \\\n",
    "                                    (new_p_i[exp_num] - samp_exp[0][exp_num].p_i).item())\n",
    "                    \n",
    "                        # Output status of optimization periodically\n",
    "                        if ((l_step + 1) % (learn_steps / 10) == 0):\n",
    "                            opt_avg = opt_perf_log.run_avg()\n",
    "                            if len(opt_perf_log.scores) < opt_perf_log.avg_window_size:\n",
    "                                opt_avg = sum(opt_perf_log.scores) / len(opt_perf_log.scores)\n",
    "                            print(f'Completed {l_step + 1} of {learn_steps} learning steps.  Loss = {opt_avg:.5f})', end='\\r')\n",
    "                    \n",
    "                    print('\\n')\n",
    "                    \n",
    "                    # Soft update target network with policy network\n",
    "                    soft_update(policy_net, target_net, opt_hyp.target_alpha)\n",
    "                    \n",
    "                    if purge_replay:\n",
    "                        buf_params = ReplayBufferHyperparameters(replay_buffer.buffer_life,\n",
    "                                                                 replay_buffer.omega,\n",
    "                                                                 replay_buffer.beta)\n",
    "                        replay_buffer = PrioritizedReplayBuffer(buf_params)\n",
    "                    \n",
    "        perf_log.add_score(score)\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % train_options.report_interval) == 0:\n",
    "            rpt_cntr = 0\n",
    "            report = f'Completed {episode + 1} episodes. Average score = '\n",
    "            avg_score = perf_log.run_avg()\n",
    "            if len(perf_log.scores) < perf_log.avg_window_size:\n",
    "                avg_score = sum(perf_log.scores) / len(perf_log.scores)\n",
    "            print(f'{report}{avg_score:.2f}')\n",
    "        \n",
    "        if ((episode >= 99) and (perf_log.run_avg() >= train_options.solved_threshold)):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break    \n",
    "        \n",
    "        if episode == (max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {max_eps} episodes')\n",
    "            break\n",
    "            \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35241",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "150edb49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 600 of 600 learning steps.  Loss = 1.94575)\n",
      "\n",
      "Completed 5 episodes. Average score = -1.20\n",
      "Completed 600 of 600 learning steps.  Loss = 0.84376)\n",
      "\n",
      "Completed 10 episodes. Average score = -0.60\n",
      "Completed 600 of 600 learning steps.  Loss = 0.56986)\n",
      "\n",
      "Completed 15 episodes. Average score = -0.33\n",
      "Completed 600 of 600 learning steps.  Loss = 0.62529)\n",
      "\n",
      "Completed 20 episodes. Average score = -0.15\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71568)\n",
      "\n",
      "Completed 25 episodes. Average score = -0.24\n",
      "Completed 600 of 600 learning steps.  Loss = 0.73778)\n",
      "\n",
      "Completed 30 episodes. Average score = -0.03\n",
      "Completed 600 of 600 learning steps.  Loss = 0.72220)\n",
      "\n",
      "Completed 35 episodes. Average score = -0.11\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71832)\n",
      "\n",
      "Completed 40 episodes. Average score = -0.07\n",
      "Completed 600 of 600 learning steps.  Loss = 0.72046)\n",
      "\n",
      "Completed 45 episodes. Average score = 0.00\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66916)\n",
      "\n",
      "Completed 50 episodes. Average score = -0.06\n",
      "Completed 600 of 600 learning steps.  Loss = 0.65239)\n",
      "\n",
      "Completed 55 episodes. Average score = -0.02\n",
      "Completed 600 of 600 learning steps.  Loss = 0.60678)\n",
      "\n",
      "Completed 60 episodes. Average score = 0.12\n",
      "Completed 600 of 600 learning steps.  Loss = 0.61723)\n",
      "\n",
      "Completed 65 episodes. Average score = 0.12\n",
      "Completed 600 of 600 learning steps.  Loss = 0.61530)\n",
      "\n",
      "Completed 70 episodes. Average score = 0.21\n",
      "Flushed replay buffer!earning steps.  Loss = 0.62582)\n",
      "Completed 600 of 600 learning steps.  Loss = 0.61972)\n",
      "\n",
      "Completed 75 episodes. Average score = 0.13\n",
      "Completed 600 of 600 learning steps.  Loss = 0.57855)\n",
      "\n",
      "Completed 80 episodes. Average score = 0.12\n",
      "Completed 600 of 600 learning steps.  Loss = 0.69931)\n",
      "\n",
      "Completed 85 episodes. Average score = 0.25\n",
      "Completed 600 of 600 learning steps.  Loss = 0.83238)\n",
      "\n",
      "Completed 90 episodes. Average score = 0.28\n",
      "Completed 600 of 600 learning steps.  Loss = 0.89805)\n",
      "\n",
      "Completed 95 episodes. Average score = 0.23\n",
      "Completed 600 of 600 learning steps.  Loss = 0.85803)\n",
      "\n",
      "Completed 100 episodes. Average score = 0.24\n",
      "Completed 600 of 600 learning steps.  Loss = 0.78132)\n",
      "\n",
      "Completed 105 episodes. Average score = 0.28\n",
      "Completed 600 of 600 learning steps.  Loss = 0.76011)\n",
      "\n",
      "Completed 110 episodes. Average score = 0.33\n",
      "Completed 600 of 600 learning steps.  Loss = 0.74201)\n",
      "\n",
      "Completed 115 episodes. Average score = 0.35\n",
      "Completed 600 of 600 learning steps.  Loss = 0.74004)\n",
      "\n",
      "Completed 120 episodes. Average score = 0.39\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71631)\n",
      "\n",
      "Completed 125 episodes. Average score = 0.47\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67790)\n",
      "\n",
      "Completed 130 episodes. Average score = 0.47\n",
      "Completed 600 of 600 learning steps.  Loss = 0.70229)\n",
      "\n",
      "Completed 135 episodes. Average score = 0.56\n",
      "Completed 600 of 600 learning steps.  Loss = 0.65074)\n",
      "\n",
      "Completed 140 episodes. Average score = 0.55\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66856)\n",
      "\n",
      "Completed 145 episodes. Average score = 0.53\n",
      "Flushed replay buffer!earning steps.  Loss = 0.62599)\n",
      "Completed 600 of 600 learning steps.  Loss = 0.60428)\n",
      "\n",
      "Completed 150 episodes. Average score = 0.58\n",
      "Completed 600 of 600 learning steps.  Loss = 0.41507)\n",
      "\n",
      "Completed 155 episodes. Average score = 0.60\n",
      "Completed 600 of 600 learning steps.  Loss = 0.62231)\n",
      "\n",
      "Completed 160 episodes. Average score = 0.56\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71053)\n",
      "\n",
      "Completed 165 episodes. Average score = 0.68\n",
      "Completed 600 of 600 learning steps.  Loss = 0.63222)\n",
      "\n",
      "Completed 170 episodes. Average score = 0.61\n",
      "Completed 600 of 600 learning steps.  Loss = 0.63862)\n",
      "\n",
      "Completed 175 episodes. Average score = 0.68\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67300)\n",
      "\n",
      "Completed 180 episodes. Average score = 0.72\n",
      "Completed 600 of 600 learning steps.  Loss = 0.63591)\n",
      "\n",
      "Completed 185 episodes. Average score = 0.66\n",
      "Completed 600 of 600 learning steps.  Loss = 0.64192)\n",
      "\n",
      "Completed 190 episodes. Average score = 0.69\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66535)\n",
      "\n",
      "Completed 195 episodes. Average score = 0.79\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67464)\n",
      "\n",
      "Completed 200 episodes. Average score = 0.85\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67028)\n",
      "\n",
      "Completed 205 episodes. Average score = 1.00\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67553)\n",
      "\n",
      "Completed 210 episodes. Average score = 1.00\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66907)\n",
      "\n",
      "Completed 215 episodes. Average score = 1.03\n",
      "Completed 600 of 600 learning steps.  Loss = 0.65667)\n",
      "\n",
      "Completed 220 episodes. Average score = 1.06\n",
      "Flushed replay buffer!earning steps.  Loss = 0.66381)\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66139)\n",
      "\n",
      "Completed 225 episodes. Average score = 1.11\n",
      "Completed 600 of 600 learning steps.  Loss = 0.52101)\n",
      "\n",
      "Completed 230 episodes. Average score = 1.14\n",
      "Completed 600 of 600 learning steps.  Loss = 0.65318)\n",
      "\n",
      "Completed 235 episodes. Average score = 1.17\n",
      "Completed 600 of 600 learning steps.  Loss = 0.59684)\n",
      "\n",
      "Completed 240 episodes. Average score = 1.26\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71085)\n",
      "\n",
      "Completed 245 episodes. Average score = 1.33\n",
      "Completed 600 of 600 learning steps.  Loss = 0.73969)\n",
      "\n",
      "Completed 250 episodes. Average score = 1.42\n",
      "Completed 600 of 600 learning steps.  Loss = 0.75857)\n",
      "\n",
      "Completed 255 episodes. Average score = 1.55\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71408)\n",
      "\n",
      "Completed 260 episodes. Average score = 1.60\n",
      "Completed 600 of 600 learning steps.  Loss = 0.69352)\n",
      "\n",
      "Completed 265 episodes. Average score = 1.53\n",
      "Completed 600 of 600 learning steps.  Loss = 0.70480)\n",
      "\n",
      "Completed 270 episodes. Average score = 1.57\n",
      "Completed 600 of 600 learning steps.  Loss = 0.71583)\n",
      "\n",
      "Completed 275 episodes. Average score = 1.67\n",
      "Completed 600 of 600 learning steps.  Loss = 0.68548)\n",
      "\n",
      "Completed 280 episodes. Average score = 1.76\n",
      "Completed 600 of 600 learning steps.  Loss = 0.66660)\n",
      "\n",
      "Completed 285 episodes. Average score = 1.76\n",
      "Completed 600 of 600 learning steps.  Loss = 0.65752)\n",
      "\n",
      "Completed 290 episodes. Average score = 1.81\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67621)\n",
      "\n",
      "Completed 295 episodes. Average score = 1.83\n",
      "Flushed replay buffer!earning steps.  Loss = 0.68175)\n",
      "Completed 600 of 600 learning steps.  Loss = 0.67475)\n",
      "\n",
      "Completed 300 episodes. Average score = 1.79\n",
      "Completed 600 of 600 learning steps.  Loss = 0.84049)\n",
      "\n",
      "Completed 305 episodes. Average score = 1.80\n",
      "Completed 600 of 600 learning steps.  Loss = 0.81860)\n",
      "\n",
      "Completed 310 episodes. Average score = 1.91\n",
      "Completed 600 of 600 learning steps.  Loss = 0.92546)\n",
      "\n",
      "Completed 315 episodes. Average score = 2.02\n",
      "Completed 600 of 600 learning steps.  Loss = 0.98337)\n",
      "\n",
      "Completed 320 episodes. Average score = 2.11\n",
      "Completed 600 of 600 learning steps.  Loss = 1.01615)\n",
      "\n",
      "Completed 325 episodes. Average score = 2.26\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05436)\n",
      "\n",
      "Completed 330 episodes. Average score = 2.37\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12271)\n",
      "\n",
      "Completed 335 episodes. Average score = 2.48\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12409)\n",
      "\n",
      "Completed 340 episodes. Average score = 2.61\n",
      "Completed 600 of 600 learning steps.  Loss = 1.11217)\n",
      "\n",
      "Completed 345 episodes. Average score = 2.88\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12009)\n",
      "\n",
      "Completed 350 episodes. Average score = 3.25\n",
      "Completed 600 of 600 learning steps.  Loss = 1.18067)\n",
      "\n",
      "Completed 355 episodes. Average score = 3.53\n",
      "Completed 600 of 600 learning steps.  Loss = 1.16281)\n",
      "\n",
      "Completed 360 episodes. Average score = 3.80\n",
      "Completed 600 of 600 learning steps.  Loss = 1.16828)\n",
      "\n",
      "Completed 365 episodes. Average score = 4.17\n",
      "Completed 600 of 600 learning steps.  Loss = 1.18451)\n",
      "\n",
      "Completed 370 episodes. Average score = 4.63\n",
      "Flushed replay buffer!earning steps.  Loss = 1.13077)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12632)\n",
      "\n",
      "Completed 375 episodes. Average score = 4.75\n",
      "Completed 600 of 600 learning steps.  Loss = 1.30044)\n",
      "\n",
      "Completed 380 episodes. Average score = 4.93\n",
      "Completed 600 of 600 learning steps.  Loss = 1.25501)\n",
      "\n",
      "Completed 385 episodes. Average score = 5.29\n",
      "Completed 600 of 600 learning steps.  Loss = 1.20684)\n",
      "\n",
      "Completed 390 episodes. Average score = 5.56\n",
      "Completed 600 of 600 learning steps.  Loss = 1.21336)\n",
      "\n",
      "Completed 395 episodes. Average score = 6.00\n",
      "Completed 600 of 600 learning steps.  Loss = 1.17335)\n",
      "\n",
      "Completed 400 episodes. Average score = 6.35\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12572)\n",
      "\n",
      "Completed 405 episodes. Average score = 6.50\n",
      "Completed 600 of 600 learning steps.  Loss = 1.13268)\n",
      "\n",
      "Completed 410 episodes. Average score = 6.85\n",
      "Completed 600 of 600 learning steps.  Loss = 1.11264)\n",
      "\n",
      "Completed 415 episodes. Average score = 7.01\n",
      "Completed 600 of 600 learning steps.  Loss = 1.08115)\n",
      "\n",
      "Completed 420 episodes. Average score = 7.09\n",
      "Completed 600 of 600 learning steps.  Loss = 1.08951)\n",
      "\n",
      "Completed 425 episodes. Average score = 7.33\n",
      "Completed 600 of 600 learning steps.  Loss = 1.09097)\n",
      "\n",
      "Completed 430 episodes. Average score = 7.70\n",
      "Completed 600 of 600 learning steps.  Loss = 1.09465)\n",
      "\n",
      "Completed 435 episodes. Average score = 7.97\n",
      "Completed 600 of 600 learning steps.  Loss = 1.10663)\n",
      "\n",
      "Completed 440 episodes. Average score = 8.33\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12095)\n",
      "\n",
      "Completed 445 episodes. Average score = 8.42\n",
      "Flushed replay buffer!earning steps.  Loss = 1.11976)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.09827)\n",
      "\n",
      "Completed 450 episodes. Average score = 8.47\n",
      "Completed 600 of 600 learning steps.  Loss = 1.07442)\n",
      "\n",
      "Completed 455 episodes. Average score = 8.47\n",
      "Completed 600 of 600 learning steps.  Loss = 1.15466)\n",
      "\n",
      "Completed 460 episodes. Average score = 8.69\n",
      "Completed 600 of 600 learning steps.  Loss = 1.24928)\n",
      "\n",
      "Completed 465 episodes. Average score = 8.58\n",
      "Completed 600 of 600 learning steps.  Loss = 1.22931)\n",
      "\n",
      "Completed 470 episodes. Average score = 8.57\n",
      "Completed 600 of 600 learning steps.  Loss = 1.25197)\n",
      "\n",
      "Completed 475 episodes. Average score = 8.84\n",
      "Completed 600 of 600 learning steps.  Loss = 1.24441)\n",
      "\n",
      "Completed 480 episodes. Average score = 8.99\n",
      "Completed 600 of 600 learning steps.  Loss = 1.25069)\n",
      "\n",
      "Completed 485 episodes. Average score = 9.10\n",
      "Completed 600 of 600 learning steps.  Loss = 1.22899)\n",
      "\n",
      "Completed 490 episodes. Average score = 9.19\n",
      "Completed 600 of 600 learning steps.  Loss = 1.20932)\n",
      "\n",
      "Completed 495 episodes. Average score = 9.25\n",
      "Completed 600 of 600 learning steps.  Loss = 1.23028)\n",
      "\n",
      "Completed 500 episodes. Average score = 9.52\n",
      "Completed 600 of 600 learning steps.  Loss = 1.21523)\n",
      "\n",
      "Completed 505 episodes. Average score = 9.74\n",
      "Completed 600 of 600 learning steps.  Loss = 1.21080)\n",
      "\n",
      "Completed 510 episodes. Average score = 9.88\n",
      "Completed 600 of 600 learning steps.  Loss = 1.18056)\n",
      "\n",
      "Completed 515 episodes. Average score = 10.18\n",
      "Completed 600 of 600 learning steps.  Loss = 1.18249)\n",
      "\n",
      "Completed 520 episodes. Average score = 10.58\n",
      "Flushed replay buffer!earning steps.  Loss = 1.19436)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.18323)\n",
      "\n",
      "Completed 525 episodes. Average score = 10.82\n",
      "Completed 600 of 600 learning steps.  Loss = 1.14516)\n",
      "\n",
      "Completed 530 episodes. Average score = 10.91\n",
      "Completed 600 of 600 learning steps.  Loss = 1.21485)\n",
      "\n",
      "Completed 535 episodes. Average score = 11.20\n",
      "Completed 600 of 600 learning steps.  Loss = 1.20567)\n",
      "\n",
      "Completed 540 episodes. Average score = 11.28\n",
      "Completed 600 of 600 learning steps.  Loss = 1.17832)\n",
      "\n",
      "Completed 545 episodes. Average score = 11.36\n",
      "Completed 600 of 600 learning steps.  Loss = 1.19229)\n",
      "\n",
      "Completed 550 episodes. Average score = 11.44\n",
      "Completed 600 of 600 learning steps.  Loss = 1.16122)\n",
      "\n",
      "Completed 555 episodes. Average score = 11.61\n",
      "Completed 600 of 600 learning steps.  Loss = 1.17721)\n",
      "\n",
      "Completed 560 episodes. Average score = 11.80\n",
      "Completed 600 of 600 learning steps.  Loss = 1.15175)\n",
      "\n",
      "Completed 565 episodes. Average score = 12.19\n",
      "Completed 600 of 600 learning steps.  Loss = 1.12391)\n",
      "\n",
      "Completed 570 episodes. Average score = 12.28\n",
      "Completed 600 of 600 learning steps.  Loss = 1.11584)\n",
      "\n",
      "Completed 575 episodes. Average score = 12.24\n",
      "Completed 600 of 600 learning steps.  Loss = 1.11047)\n",
      "\n",
      "Completed 580 episodes. Average score = 12.36\n",
      "Completed 600 of 600 learning steps.  Loss = 1.08227)\n",
      "\n",
      "Completed 585 episodes. Average score = 12.45\n",
      "Completed 600 of 600 learning steps.  Loss = 1.11180)\n",
      "\n",
      "Completed 590 episodes. Average score = 12.84\n",
      "Completed 600 of 600 learning steps.  Loss = 1.10579)\n",
      "\n",
      "Completed 595 episodes. Average score = 12.94\n",
      "Flushed replay buffer!earning steps.  Loss = 1.09773)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.08322)\n",
      "\n",
      "Completed 600 episodes. Average score = 12.76\n",
      "Completed 600 of 600 learning steps.  Loss = 0.86861)\n",
      "\n",
      "Completed 605 episodes. Average score = 12.74\n",
      "Completed 600 of 600 learning steps.  Loss = 1.08852)\n",
      "\n",
      "Completed 610 episodes. Average score = 12.67\n",
      "Completed 600 of 600 learning steps.  Loss = 1.00097)\n",
      "\n",
      "Completed 615 episodes. Average score = 12.59\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02249)\n",
      "\n",
      "Completed 620 episodes. Average score = 12.55\n",
      "Completed 600 of 600 learning steps.  Loss = 1.03451)\n",
      "\n",
      "Completed 625 episodes. Average score = 12.50\n",
      "Completed 600 of 600 learning steps.  Loss = 1.01972)\n",
      "\n",
      "Completed 630 episodes. Average score = 12.52\n",
      "Completed 600 of 600 learning steps.  Loss = 1.07786)\n",
      "\n",
      "Completed 635 episodes. Average score = 12.53\n",
      "Completed 600 of 600 learning steps.  Loss = 1.07985)\n",
      "\n",
      "Completed 640 episodes. Average score = 12.60\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05805)\n",
      "\n",
      "Completed 645 episodes. Average score = 12.66\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05389)\n",
      "\n",
      "Completed 650 episodes. Average score = 12.71\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05598)\n",
      "\n",
      "Completed 655 episodes. Average score = 12.70\n",
      "Completed 600 of 600 learning steps.  Loss = 1.06935)\n",
      "\n",
      "Completed 660 episodes. Average score = 12.61\n",
      "Completed 600 of 600 learning steps.  Loss = 1.04696)\n",
      "\n",
      "Completed 665 episodes. Average score = 12.42\n",
      "Completed 600 of 600 learning steps.  Loss = 1.03454)\n",
      "\n",
      "Completed 670 episodes. Average score = 12.39\n",
      "Flushed replay buffer!earning steps.  Loss = 1.04556)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02739)\n",
      "\n",
      "Completed 675 episodes. Average score = 12.40\n",
      "Completed 600 of 600 learning steps.  Loss = 0.87723)\n",
      "\n",
      "Completed 680 episodes. Average score = 12.37\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05160)\n",
      "\n",
      "Completed 685 episodes. Average score = 12.42\n",
      "Completed 600 of 600 learning steps.  Loss = 0.99803)\n",
      "\n",
      "Completed 690 episodes. Average score = 12.10\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02133)\n",
      "\n",
      "Completed 695 episodes. Average score = 12.01\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02370)\n",
      "\n",
      "Completed 700 episodes. Average score = 12.19\n",
      "Completed 600 of 600 learning steps.  Loss = 1.01982)\n",
      "\n",
      "Completed 705 episodes. Average score = 12.48\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05797)\n",
      "\n",
      "Completed 710 episodes. Average score = 12.68\n",
      "Completed 600 of 600 learning steps.  Loss = 1.03564)\n",
      "\n",
      "Completed 715 episodes. Average score = 12.79\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02705)\n",
      "\n",
      "Completed 720 episodes. Average score = 12.71\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02602)\n",
      "\n",
      "Completed 725 episodes. Average score = 12.69\n",
      "Completed 600 of 600 learning steps.  Loss = 1.04882)\n",
      "\n",
      "Completed 730 episodes. Average score = 12.62\n",
      "Completed 600 of 600 learning steps.  Loss = 1.03876)\n",
      "\n",
      "Completed 735 episodes. Average score = 12.40\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02056)\n",
      "\n",
      "Completed 740 episodes. Average score = 12.41\n",
      "Completed 600 of 600 learning steps.  Loss = 1.05320)\n",
      "\n",
      "Completed 745 episodes. Average score = 12.51\n",
      "Flushed replay buffer!earning steps.  Loss = 1.04288)\n",
      "Completed 600 of 600 learning steps.  Loss = 1.04543)\n",
      "\n",
      "Completed 750 episodes. Average score = 12.53\n",
      "Completed 600 of 600 learning steps.  Loss = 0.81012)\n",
      "\n",
      "Completed 755 episodes. Average score = 12.57\n",
      "Completed 600 of 600 learning steps.  Loss = 0.92899)\n",
      "\n",
      "Completed 760 episodes. Average score = 12.51\n",
      "Completed 600 of 600 learning steps.  Loss = 0.94100)\n",
      "\n",
      "Completed 765 episodes. Average score = 12.43\n",
      "Completed 600 of 600 learning steps.  Loss = 1.04526)\n",
      "\n",
      "Completed 770 episodes. Average score = 12.73\n",
      "Completed 600 of 600 learning steps.  Loss = 1.04836)\n",
      "\n",
      "Completed 775 episodes. Average score = 12.88\n",
      "Completed 600 of 600 learning steps.  Loss = 1.02636)\n",
      "\n",
      "Completed 780 episodes. Average score = 12.86\n",
      "Completed 600 of 600 learning steps.  Loss = 1.01797)\n",
      "\n",
      "Completed 785 episodes. Average score = 12.83\n",
      "Completed 600 of 600 learning steps.  Loss = 1.00785)\n",
      "\n",
      "Completed 790 episodes. Average score = 12.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 600 of 600 learning steps.  Loss = 1.00574)\n",
      "\n",
      "Completed 795 episodes. Average score = 13.07\n",
      "Solved with average score of 13.070000000000014 in 795 episodes\n"
     ]
    }
   ],
   "source": [
    "model = ParameterizedModel()\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(model,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c3d4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model,\"end_training_one.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f20318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338645f",
   "metadata": {},
   "source": [
    "### Demo\n",
    "The cell below can be run to demonstrate the behavior of the trained agent.  It is necessary to first run all cells except the training cell.  One can also review this [video](https://youtu.be/ieCMukh9Rwg) of the trained agent's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653ee079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: score = 7.0\r"
     ]
    }
   ],
   "source": [
    "demo_eps = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demo_net = torch.load('submission_net.pt')\n",
    "demo_net = demo_net.to(device)\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "for episode in range(demo_eps):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    done = False\n",
    "    while done == False:\n",
    "            \n",
    "            # Environment interaction\n",
    "            action = sel_action(demo_net, state, device)\n",
    "            env_info = env.step(int(action.cpu()))[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # Process interaction results\n",
    "            score += reward\n",
    "            if not (reward == 0):\n",
    "                print(f'Episode {episode+1}: score = {score}', end='\\r')\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a759644",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The `n_step_order` hyperparameter strongly influenced the outcome.  If too low, the signals from anything except a banana immediately in front of the agent appeared to get washed together, preventing effective learning of even short range behavior.  If `n_step_order` were too high, a similar state value washout seemed to happen at longer distances.  With high `n_step_order` there also tended to be very quick learning, getting near solve performance after about 100 episodes, but then progressive and continuous decline, after briefly peaking.  Care has to be taken that experiences in the replay buffer do not become too 'stale' with higher order, since the direct reward terms are those observed at a particular time, but the policy is continuously changing.<br>\n",
    "\n",
    "The score is not necessarily the best metric to assess the progress of training.  Probably on many runs before success, the process was terminated too early, since the score did not seem to be improving.  However, close evaluation of the agent's behavior typically demonstrates progressive addressing of the issues with 'short-cut' solutions.  For example, it was common early in training that blue bananas would be hit a few times in random forward moves, resulting in the agent only backing up.  It can take a lot episodes for the sparse signal, that going backwards all the time can have a similar result, to effect appropriate forward vs. backward balance.  Once the basic balance problems were resolved, scores increased very quickly.  They then slowly leveled off as behavior in more infrequently occurring situations was addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed728e6",
   "metadata": {},
   "source": [
    "### Ideas for Future Work\n",
    "+ Delay-line memory / state augmentation: A common pitfall encountered in the agent policy, after some initial progress, was that the agent would sometimes turn around in a circle without stopping, until the end of an episode.  This was evidently because no direction away from its position looked safe, so it would keep turning in search of a safe direction to travel.  A lot of environment time seemed to be wasted in redundant experiences, since there is little additional information being supplied after the first full look-around.  It seems that concatenating some state history, perhaps the last 5 or 6 sets of state observables, as well as the selected actions, onto the input to the parameterized function could improve the situation.  This measure would make it possible for the agent to, in principle, detect when it has done a full revolution without moving anywhere, and possibly learn to take a less safe looking forward move in such a situation.\n",
    "+ Performance based noise scheduling: The initial noise magnitude was a parameter that needed to be updated in response to many other hyperparameter changes, in order for learning to be possible at all.  Too little initial noise, and the agent would become locked in some simple initial behavior.  But if there is too much initial noise, improvement can take a very long time.  This is because the noise magnitudes are a trainable parameters, and training updates will not lower these amplitudes until the network starts achieving performance beyond random-policy.  It seems sensible to implement a schedule, in which the noise amplitudes are globally reduced from a starting value, until some minor performance of the agent is evident.  Following this, the normal learnable noise magnitude parameter mechanism can take over.  The noise might also be globally increased when the performance trajectory suggests overfitting (peaking and subsequent decline in performance).\n",
    "+ Ensemble learning: A very common phenomenon was that an agent would learn to favor turning one way or the other (left/right) and eventually never turn in the non-dominant direction, even when it was possible to save time doing so.  At least one agent learned a work around after learning to only ever turn left.  When this agent detected a yellow banana passing on the right, it would go past it, turn left, and then back up to get it.  It is possible this inefficient habituation might be addressed by training multiple networks at the same time on the experience data, and setting up some kinda of competitive feedback.  Perhaps each network in the ensemble would get to operate the agent for a whole episode in round-robin fashion, then the other networks would be directly trained on the full output of the 'operator' network, according to its field performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n",
    "[5] Fortunato et. al., Noisy Networks for Exploration, arXiv:1706.10295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
