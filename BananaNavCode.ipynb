{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/bannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5c8e",
   "metadata": {},
   "source": [
    "### Supplemental Packages\n",
    "Run the following code cell *once* to install additional packages required by the implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "\"C:\\Projects\\Value-based-methods\\p1_navigation\\Banana_Windows_x86_64\\Banana_Windows_x86_64\\Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $s$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "<BR>\n",
    "\n",
    "The neural network will be a fully connected MLP with one input for each state variable, at least one hidden layer, and one output for each pair $(z_i, a)$.  See the implentation section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47142b86",
   "metadata": {},
   "source": [
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameterized model.  Like in [1], the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\omega}, \\omega \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\omega$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [3].\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884e3f",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "`state_dimension`: Dimension of the observable state space<br>\n",
    "`num_actions`: Number of actions available to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a5746ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentHyperparameters():\n",
    "    def __init__(self,state_dimension=37,num_actions=4):\n",
    "        self.state_dimension = state_dimension\n",
    "        self.num_actions = num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43249fb8",
   "metadata": {},
   "source": [
    "#### Parameterized Model\n",
    "`num_hidden_layers`: How many hidden layers are included in the neural network model<br>\n",
    "`hidden_layer_size`: How many neurpns are included in each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f239be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters():\n",
    "    def __init__(self,num_hidden_layers=1,hidden_layer_size=200):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c93654",
   "metadata": {},
   "source": [
    "#### Reward Distribution\n",
    "`v_min`: Lower limit clipping value for *n-step* return<br>\n",
    "`v_max`: Upper limit clipping value for *n-step* return<br>\n",
    "`gamma`: Discount factor for reward calculation.  An expected reward n steps in the future is discounted by $\\gamma^n$<br>\n",
    "`n_step_order`: How many environment steps from initial state are considered in constructing action value distribution<br>\n",
    "`n_atoms`: Number of discretization bins for representing possible value distribution returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e717b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionHyperparameters():\n",
    "    def __init__(self,v_min=-5,v_max=5,gamma=1,n_step_order=10,n_atoms=51):\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.gamma = gamma\n",
    "        self.n_step_order = n_step_order\n",
    "        self.n_atoms = n_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782dd24",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "`buffer_size`: Maximum number of transition events that may be stored in the replay buffer<br>\n",
    "`omega`: Loss influence factor for transition event selection probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed4744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferHyperparameters():\n",
    "    def __init__(self,buffer_size=1e6,omega=0.5):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.omega = omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dff3c",
   "metadata": {},
   "source": [
    "#### Optimization Behavior\n",
    "`lr`: Learning rate for parameterized model optimizer<br>\n",
    "`learn_interval`: Number of environment exploration steps between each learning phase<br>\n",
    "`batch_size`: Number of n-step transition events to process during each learning phase <br>\n",
    "`target_alpha`: Soft update factor used so that target reward distribution lags policy reward distribution.  See [this explanation](https://deeplizard.com/learn/video/xVkPh9E9GfE) for why this is necessary.  The following update will be applied once per batch:<br><br>\n",
    "$$\\theta_{target} = (1-\\alpha)\\theta_{target} + \\alpha\\theta_{policy}$$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb02b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHyperparameters():\n",
    "    def __init__(self,lr=1e-4,learn_interval=50,batch_size=32,target_alpha=1e-2):\n",
    "        self.lr = lr\n",
    "        self.learn_interval = learn_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.target_alpha = target_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484595f5",
   "metadata": {},
   "source": [
    "Like in [3], a sum tree structure is used for efficient weighted sampling over a large number of items.  This implementation is heavily based on that in reference [4](http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/), but with some organizational changes to accomodate dynamic resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that represents an experience in the experience buffer and/or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def remove(self):\n",
    "        if self.parent is not None:  # remove only has effect if there is a parent\n",
    "            if self.parent.left_child is self:\n",
    "                sibling = self.parent.right_child\n",
    "            else:\n",
    "                sibling = self.parent.left_child\n",
    "            # Parent will become a leaf, clone data from sibling and update references\n",
    "            self.parent.data = sibling.data\n",
    "            self.parent.update_p(sibling.p_i - self.parent.p_i)\n",
    "            self.parent.left_child, self.parent.right_child = None, None\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "            \n",
    "    # This is used to identify low sampling probability nodes for removal, and determine\n",
    "    # the lowest p_i, which is used to normalize the magnitude of parameterized model updates\n",
    "    def minimal_node(self):\n",
    "        if self.data is not None:\n",
    "            return self\n",
    "        else:\n",
    "            min_left = self.left_child.minimal_node()\n",
    "            min_right = self.right_child.minimal_node()\n",
    "            return min_left if min_left.p_i < min_right.p_i else min_right\n",
    "        \n",
    "    def describe(self):\n",
    "        desc_str = f'{\"Non-leaf\" if self.data is None else str(self.data)}: {str(self.p_i)}'\n",
    "        if self.left_child is not None:\n",
    "            left_desc_str = \\\n",
    "                f'{\"Non-leaf\" if self.left_child.data is not None else str(self.left_child.data)}' \\\n",
    "                +f': {self.left_child.p_i}'\n",
    "        else:\n",
    "            left_desc_str = \"Nothing\"\n",
    "        if self.right_child is not None:\n",
    "            right_desc_str = \\\n",
    "                f'{\"Non-leaf\" if self.right_child.data is not None else str(self.right_child.data)}' \\\n",
    "                +f': {self.right_child.p_i}'\n",
    "        else:\n",
    "            right_desc_str = \"Nothing\"\n",
    "        print(f'{desc_str} --> {left_desc_str} | {right_desc_str}')\n",
    "        if self.left_child is not None:\n",
    "            self.left_child.describe()\n",
    "        if self.right_child is not None:\n",
    "            self.right_child.describe()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95526535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized replay buffer definition\n",
    "\n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "\n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,replay_buffer_hyperparameters=ReplayBufferHyperparameters()):\n",
    "        self.max_size = replay_buffer_hyperparameters.buffer_size\n",
    "        self.omega = replay_buffer_hyperparameters.omega\n",
    "        self.store = SumTreeNode()\n",
    "        self.current_size = 0\n",
    "        self.min_p_i = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.current_size \n",
    "        \n",
    "    def add_experience(self, experience, loss):\n",
    "        if self.current_size >= self.max_size:    # Remove minimum p_i experience\n",
    "            self.store.minimal_node().remove()\n",
    "        self.store.attach_child(SumTreeNode(experience, pow(loss,self.omega)))\n",
    "        self.min_p_i = self.store.minimal_node().p_i\n",
    "        if self.current_size < self.max_size:\n",
    "            self.current_size += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        if batch_size > self.current_size:\n",
    "            return None\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        return [self.store.weighted_retrieve(p_samp) for p_samp in sample_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "068dc3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Counts\n",
      "one: 0.0\n",
      "two: 2.384976525821596\n",
      "three: 3.2300469483568075\n",
      "four: 1.0\n",
      "five: 2.187793427230047\n",
      "six: 3.2629107981220655\n",
      "seven: 1.215962441314554\n",
      "eight: 2.291079812206573\n",
      "nine: 3.2065727699530515\n"
     ]
    }
   ],
   "source": [
    "# Unit test for PrioritizedReplayBuffer\n",
    "test_rep_hyp = ReplayBufferHyperparameters(buffer_size = 8, omega = 1)\n",
    "test_rep_buffer = PrioritizedReplayBuffer(test_rep_hyp)\n",
    "test_experiences = ['one','two','three','four','five','six','seven','eight','nine']\n",
    "test_weights = [0.99,2,3,1,2,3,1,2,3]\n",
    "test_batch_size = 4\n",
    "test_batches = 1000\n",
    "for (e,wt) in zip(test_experiences,test_weights):\n",
    "    test_rep_buffer.add_experience(e,wt)\n",
    "test_samples = []\n",
    "for i in range(test_batches):\n",
    "    test_samples += [sample.data for sample in test_rep_buffer.sample(test_batch_size)]\n",
    "print(\"Normalized Counts\")\n",
    "for exp in test_experiences:\n",
    "    print(f'{exp}: {test_samples.count(exp) / test_samples.count(\"four\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e6e6",
   "metadata": {},
   "source": [
    "The linear layers have noisy bias and weight components, implemented with factored gaussian noise according to [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_init=0.5):\n",
    "        super().__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            noise_init*torch.ones(in_dim,out_dim,dtype=torch.float64,requires_grad=True)\n",
    "        self.noisy_bias = noise_init*torch.ones(1,out_dim,dtype=torch.float64,requires_grad=True)\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [5]\n",
    "        return torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized gaussian noise\n",
    "        in_dim_noise = self.noise_transform(torch.randn((self.noisy_weights.Size(dim=0),1)))\n",
    "        out_dim_noise = self.noise_transform(torch.randn((1,self.noisy_weights.Size(dim=1))))\n",
    "        \n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "        \n",
    "        return self.deterministic_linear(x) + torch.matmul(weight_noise,x) + bias_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729af8",
   "metadata": {},
   "source": [
    "#### Training Session Options\n",
    "`max_episodes`: Maximum number of episodes before stopping training run<br>\n",
    "`rewards_buffer`: Pass in empty list or existing list.  Average rewards per episode data is appended to this list<br>\n",
    "`reward_window`: Length of averaging window for reporting rewards, in number of episodes<br>\n",
    "`report_interval`: Interval in episodes for updating `rewards_buffer` and displaying status<br>\n",
    "`solved_threshold`: Average reward over averaging window required to stop training early<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOptions():\n",
    "    def __init__(self,max_episodes=2000,\n",
    "                 rewards_buffer=[],reward_window=100,\n",
    "                 report_interval=50,solved_threshold=13):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.rewards_buffer = rewards_buffer\n",
    "        self.reward_window = reward_window\n",
    "        self.report_interval = report_interval\n",
    "        self.solved_threshold = solved_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73fa6",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`net`: Parameterized model to use for training run<br>\n",
    "`replay_buffer`: Replay buffer object to use for training run<br>\n",
    "`optimization_hyperparameters`: `OptimizationHyperparameters` object to use for training run<br>\n",
    "`train_options`: `TrainingOptions` object to use for training run\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to the `rewards_buffer` of the supplied `train_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee256af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n",
    "[5] Fortunato et. al., Noisy Networks for Exploration, arXiv:1706.10295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
