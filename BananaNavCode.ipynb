{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/bannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file.\n",
    "+ Clone this repository into the `p1_navigation/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5c8e",
   "metadata": {},
   "source": [
    "### Supplemental Packages\n",
    "Run the following code cell *once* to install additional packages required by the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $S$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "<BR>\n",
    "\n",
    "The neural network will be a fully connected MLP with one input for each state variable, at least one hidden layer, and one output for each pair $(z_i, a)$.  See the implentation section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47142b86",
   "metadata": {},
   "source": [
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameterized model.  Like in [1], the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\omega}, \\omega \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\omega$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [3].\n",
    "<br><br>Qualitatively, the $Loss$ is this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43249fb8",
   "metadata": {},
   "source": [
    "#### Parameterized Model\n",
    "`num_hidden_layers`: How many hidden layers are included in the neural network model<br>\n",
    "`hidden_layer_size`: How many neurpns are included in each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f239be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters():\n",
    "    def __init__(self,num_hidden_layers=1,hidden_layer_size=200):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c93654",
   "metadata": {},
   "source": [
    "#### Reward Distribution\n",
    "`v_min`: Lower limit clipping value for *n-step* return<br>\n",
    "`v_max`: Upper limit clipping value for *n-step* return<br>\n",
    "`gamma`: Discount factor for reward calculation.  An expected reward n steps in the future is discounted by $\\gamma^n$<br>\n",
    "`n_step_order`: How many environment steps from initial state are considered in constructing action value distribution<br>\n",
    "`n_atoms`: Number of discretization bins for representing possible value distribution returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e717b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionHyperparameters():\n",
    "    def __init__(self,v_min=-5,v_max=5,gamma=1,n_step_order=10,n_atoms=51):\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.gamma = gamma\n",
    "        self.n_step_order = n_step_order\n",
    "        self.n_atoms = n_atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782dd24",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "`buffer_size`: Maximum number of transition events that may be stored in the replay buffer<br>\n",
    "`omega`: Loss influence factor for transition event selection probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferHyperparameters():\n",
    "    def __init__(self,buffer_size=1e6,omega=0.5):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.omega = omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dff3c",
   "metadata": {},
   "source": [
    "#### Optimization Behavior\n",
    "`lr`: Learning rate for parameterized model optimizer<br>\n",
    "`learn_interval`: Number of environment exploration steps between each learning phase<br>\n",
    "`batch_size`: Number of n-step transition events to process during each learning phase <br>\n",
    "`target_alpha`: Soft update factor used so that target reward distribution lags policy reward distribution.  See [this explanation](https://deeplizard.com/learn/video/xVkPh9E9GfE) for why this is necessary.  The following update will be applied once per batch:<br><br>\n",
    "$$\\theta_{target} = (1-\\alpha)\\theta_{target} + \\alpha\\theta_{policy}$$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb02b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHyperparameters():\n",
    "    def __init__(self,lr=1e-4,learn_interval=50,batch_size=32,target_alpha=1e-2):\n",
    "        self.lr = lr\n",
    "        self.learn_interval = learn_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.target_alpha = target_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729af8",
   "metadata": {},
   "source": [
    "#### Training Session Options\n",
    "`max_episodes`: Maximum number of episodes before stopping training run<br>\n",
    "`rewards_buffer`: Pass in empty list or existing list.  Average rewards per episode data is appended to this list<br>\n",
    "`reward_window`: Length of averaging window for reporting rewards, in number of episodes<br>\n",
    "`report_interval`: Interval in episodes for updating `rewards_buffer` and displaying status<br>\n",
    "`solved_threshold`: Average reward over averaging window required to stop training early<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c86e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOptions():\n",
    "    def __init__(self,max_episodes=2000,\n",
    "                 rewards_buffer=[],reward_window=100,\n",
    "                 report_interval=50,solved_threshold=13):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.rewards_buffer = rewards_buffer\n",
    "        self.reward_window = reward_window\n",
    "        self.report_interval = report_interval\n",
    "        self.solved_threshold = solved_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73fa6",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`net`: Parameterized model to use for training run<br>\n",
    "`replay_buffer`: Replay buffer object to use for training run<br>\n",
    "`optimization_hyperparameters`: `OptimizationHyperparameters` object to use for training run<br>\n",
    "`train_options`: `TrainingOptions` object to use for training run\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to the `rewards_buffer` of the supplied `train_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee256af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
