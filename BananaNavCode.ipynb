{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/bannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "\"C:\\Projects\\Value-based-methods\\p1_navigation\\Banana_Windows_x86_64\\Banana_Windows_x86_64\\Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $s$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "<BR>\n",
    "\n",
    "The neural network will be a fully connected MLP with one input for each state variable, at least one hidden layer, and one output for each pair $(z_i, a)$.  See the implentation section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47142b86",
   "metadata": {},
   "source": [
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameterized model.  Like in [1], the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\omega}, \\omega \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\omega$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [3].\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884e3f",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "`state_dimension`: Dimension of the observable state space<br>\n",
    "`num_actions`: Number of actions available to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5746ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentHyperparameters():\n",
    "    def __init__(self,state_dimension=37,num_actions=4):\n",
    "        self.state_dimension = state_dimension\n",
    "        self.num_actions = num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43249fb8",
   "metadata": {},
   "source": [
    "#### Parameterized Model\n",
    "`num_hidden_layers`: How many hidden layers are included in the neural network model<br>\n",
    "`hidden_layer_size`: How many neurons are included in each hidden layer<br>\n",
    "`noise_init`: Initial standard deviation for all noise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f239be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters():\n",
    "    def __init__(self,num_hidden_layers=3,hidden_layer_size=200,noise_init=0.05):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.noise_init = noise_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c93654",
   "metadata": {},
   "source": [
    "#### Reward Distribution\n",
    "`v_min`: Lower limit clipping value for *n-step* return<br>\n",
    "`v_max`: Upper limit clipping value for *n-step* return<br>\n",
    "`gamma`: Discount factor for reward calculation.  An expected reward n steps in the future is discounted by $\\gamma^n$<br>\n",
    "`n_step_order`: How many environment steps from initial state are considered in constructing action value distribution<br>\n",
    "`n_atoms`: Number of discretization bins for representing possible value distribution returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionHyperparameters():\n",
    "    def __init__(self,v_min=-15,v_max=15,gamma=0.85,n_step_order=5,n_atoms=51):\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.gamma = gamma\n",
    "        self.n_step_order = n_step_order\n",
    "        self.n_atoms = n_atoms\n",
    "        self.z_i = \\\n",
    "            torch.tensor([(v_min + (i * ((v_max-v_min) / (n_atoms - 1)))) for i in range(n_atoms)])\n",
    "        self.z_i = torch.unsqueeze(self.z_i,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782dd24",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "`buffer_life`: Buffer will be reset after this many sample() calls<br>\n",
    "`omega`: Loss influence factor for transition event selection probabilities<br>\n",
    "`beta`: Importance sampling correction coefficient.  See [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferHyperparameters():\n",
    "    def __init__(self,buffer_life=(50*300), omega=0.6, beta=1.0):\n",
    "        self.buffer_life = buffer_life\n",
    "        self.omega = omega\n",
    "        self.beta = beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dff3c",
   "metadata": {},
   "source": [
    "#### Optimization Behavior\n",
    "`lr`: Learning rate for parameterized model optimizer<br>\n",
    "`learn_interval`: Number of environment exploration steps between each learning phase<br>\n",
    "`batch_size`: Number of n-step transition events to process during each learning phase <br>\n",
    "`target_alpha`: Soft update factor used so that target reward distribution lags policy reward distribution.  See [this explanation](https://deeplizard.com/learn/video/xVkPh9E9GfE) for why this is necessary.  The following update will be applied once per batch:<br><br>\n",
    "$$\\theta_{target} = (1-\\alpha)\\theta_{target} + \\alpha\\theta_{policy}$$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb02b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHyperparameters():\n",
    "    def __init__(self,lr=0.00002,learn_interval=1,batch_size=64,target_alpha=7e-4):\n",
    "        self.lr = lr\n",
    "        self.learn_interval = learn_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.target_alpha = target_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484595f5",
   "metadata": {},
   "source": [
    "Like in [3], a sum tree structure is used for efficient weighted sampling over a large number of items.  This implementation is heavily based on that in reference [4](http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/), but with some organizational changes to accomodate dynamic resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that represents an experience in the experience buffer and/or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def remove(self):\n",
    "        if self.parent is not None:  # remove only has effect if there is a parent\n",
    "            if self.parent.left_child is self:\n",
    "                sibling = self.parent.right_child\n",
    "            else:\n",
    "                sibling = self.parent.left_child\n",
    "            # Clone data to parent from sibling and update references\n",
    "            if sibling is None:\n",
    "                print(f'Removing:{self.data} |||| {self.p_i}')\n",
    "                print(f'Parent:{self.parent.data} |||| {self.parent.p_i}')\n",
    "                print(f'Sibling:{sibling.data} |||| {sibling.p_i}')\n",
    "            self.parent.data = sibling.data\n",
    "            self.parent.left_child = sibling.left_child\n",
    "            self.parent.right_child = sibling.right_child\n",
    "            if (self.parent.left_child is not None):\n",
    "                self.parent.left_child.parent = self.parent\n",
    "            if (self.parent.right_child is not None):\n",
    "                self.parent.right_child.parent = self.parent\n",
    "            self.parent.update_p(sibling.p_i - self.parent.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "            \n",
    "    # This is used to identify low sampling probability nodes for removal\n",
    "    def minimal_node(self):\n",
    "        if self.data is not None:\n",
    "            return self\n",
    "        else:\n",
    "            min_left = self.left_child.minimal_node()\n",
    "            min_right = self.right_child.minimal_node()\n",
    "            return min_left if min_left.p_i < min_right.p_i else min_right  \n",
    "       \n",
    "    # Debug function\n",
    "    def check_node(self):\n",
    "        if self.data is not None:\n",
    "            return \"\"\n",
    "        if ((self.left_child is None) or (self.right_child is None)):\n",
    "            return f'Error: [{self.data}||{self.p_i}]'\n",
    "        left_check = self.left_child.check_node()\n",
    "        right_check = self.right_child.check_node()\n",
    "        if ((len(left_check) > 0) or (len(right_check) > 0)):\n",
    "            return f'[{self.data}||{self.p_i}]-->{self.left_child.check_node()}{self.right_child.check_node()}'\n",
    "        return \"\"\n",
    "    \n",
    "    # String representation of tree\n",
    "    def tree_str(self,lvl=0):\n",
    "        lvl_str= '--> '*lvl\n",
    "        left_str = self.left_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        right_str = self.right_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        par_str = self.parent.p_i if self.parent is not None else 'nothing'\n",
    "        my_str = f'{lvl_str}[{self.data}|{self.p_i}] under [{par_str}]\\n'\n",
    "        return f'{my_str}{left_str}{right_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95526535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized replay buffer definition\n",
    "\n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "\n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,replay_buffer_hyperparameters=ReplayBufferHyperparameters()):\n",
    "        self.buffer_life = replay_buffer_hyperparameters.buffer_life\n",
    "        self.omega = replay_buffer_hyperparameters.omega\n",
    "        self.store = SumTreeNode()\n",
    "        self.sample_count = 0\n",
    "        self.exp_count = 0\n",
    "        self.beta = replay_buffer_hyperparameters.beta\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.exp_count\n",
    "    \n",
    "    def add_experience(self, experience, loss):\n",
    "        new_p_i = pow(loss, self.omega)\n",
    "        self.store.attach_child(SumTreeNode(experience, new_p_i))\n",
    "        self.exp_count += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        samples = ([self.store.weighted_retrieve(p_samp) for p_samp in sample_keys],\n",
    "                   (self.exp_count,self.beta,self.store.p_i))\n",
    "        self.sample_count += 1\n",
    "        if (self.sample_count >= self.buffer_life):\n",
    "            self.sample_count = 0\n",
    "            self.store = SumTreeNode()\n",
    "            self.exp_count = 0\n",
    "            print ('Flushed replay buffer!')\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068dc3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test for PrioritizedReplayBuffer\n",
    "test_rep_hyp = ReplayBufferHyperparameters(buffer_life = 2000, omega = 1)\n",
    "test_rep_buffer = PrioritizedReplayBuffer(test_rep_hyp)\n",
    "test_experiences = ['one','two','three','four','five','six','seven','eight','nine']\n",
    "test_weights = [0.99,2,3,1,2,3,1,2,3]\n",
    "test_batch_size = 4\n",
    "test_batches = 1000\n",
    "for (e,wt) in zip(test_experiences,test_weights):\n",
    "    test_rep_buffer.add_experience(e,wt)\n",
    "test_samples = []\n",
    "for i in range(test_batches):\n",
    "    test_samples += [sample.data for sample in test_rep_buffer.sample(test_batch_size)[0]]\n",
    "print(\"Normalized Counts\")\n",
    "for exp in test_experiences:\n",
    "    print(f'{exp}: {test_samples.count(exp) / test_samples.count(\"four\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e6e6",
   "metadata": {},
   "source": [
    "The linear layers have noisy bias and weight components, implemented with factored gaussian noise according to [5].  The standard deviation of each noise component is subject to optimization (learning) by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_init=0.05):\n",
    "        super(NoisyLinear,self).__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            nn.Parameter(noise_init*torch.ones(in_dim,out_dim,dtype=torch.float32))\n",
    "        self.noisy_bias = nn.Parameter(noise_init*torch.ones(1,out_dim,dtype=torch.float32))\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [5]\n",
    "        return torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized gaussian noise, clamping to +/- 3 sigma\n",
    "        in_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((self.noisy_weights.size(dim=0),1),device=x.device),-3.0,3.0))\n",
    "        out_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((1,self.noisy_weights.size(dim=1)),device=x.device),-3.0,3.0))\n",
    "        \n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "    \n",
    "        x = x.float()\n",
    "        return self.deterministic_linear(x) + torch.matmul(x,weight_noise) + bias_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319ce3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test for NoisyLinear\n",
    "test_noisy_layer = NoisyLinear(10,4)\n",
    "test_input_data = torch.ones((5,10))\n",
    "test_output = test_noisy_layer(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76476cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterized Model Implementation\n",
    "class ParameterizedModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 environment_hyperparameters = EnvironmentHyperparameters(),\n",
    "                 model_hyperparameters = ModelHyperparameters(),\n",
    "                 distribution_hyperparameters = DistributionHyperparameters()\n",
    "                ):\n",
    "        super(ParameterizedModel,self).__init__()\n",
    "        in_dim = environment_hyperparameters.state_dimension\n",
    "        out_dim = environment_hyperparameters.num_actions \\\n",
    "                  * distribution_hyperparameters.n_atoms\n",
    "        hidden_size = model_hyperparameters.hidden_layer_size\n",
    "        num_hidden_layers = model_hyperparameters.num_hidden_layers\n",
    "        noise_init = model_hyperparameters.noise_init\n",
    "        \n",
    "        self.distribution_hyperparameters = distribution_hyperparameters\n",
    "        self.environment_hyperparameters = environment_hyperparameters\n",
    "        self.model_hyperparameters = model_hyperparameters\n",
    "        self.num_actions = environment_hyperparameters.num_actions\n",
    "        self.n_atoms = distribution_hyperparameters.n_atoms\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(NoisyLinear(in_dim,hidden_size,noise_init))\n",
    "        layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        for i in range(num_hidden_layers-1):\n",
    "            layers.append(NoisyLinear(hidden_size,hidden_size,noise_init))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.05))\n",
    "        layers.append(NoisyLinear(hidden_size,out_dim,noise_init))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.reg_layers(x)\n",
    "        \n",
    "        # See reference [1], softmax must be calculated individually on the\n",
    "        # atoms for each action since each action is a probability distribution\n",
    "        y = torch.tensor([],requires_grad=True,device=x.device)\n",
    "        for i in range(self.num_actions):\n",
    "            y = torch.hstack((y,F.softmax(x[:,(i*self.n_atoms):((i+1)*self.n_atoms)],dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4908a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test for ParameterizedModel\n",
    "test_env_hyp = EnvironmentHyperparameters(11,3)\n",
    "test_mdl_hyp = ModelHyperparameters(3,30)\n",
    "test_dst_hyp = DistributionHyperparameters(n_atoms = 7)\n",
    "test_mdl = ParameterizedModel(test_env_hyp, test_mdl_hyp, test_dst_hyp)\n",
    "test_input_data = torch.randn(5,11)\n",
    "test_output = test_mdl(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic function\n",
    "def tensor_check(input,desc):\n",
    "    if torch.any(torch.isnan(input)):\n",
    "        print(f'NaNs in {desc}')\n",
    "    if torch.any(torch.isinf(input)):\n",
    "        print(f'Inf in {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_check unit test\n",
    "A = torch.tensor([3,0,1])\n",
    "B = torch.tensor([2,0,1])\n",
    "C = torch.tensor([1,1,1])\n",
    "tensor_check(torch.div(A,B),\"A/B\")\n",
    "tensor_check(torch.div(C,B),\"C/B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca8356",
   "metadata": {},
   "source": [
    "See algorithm 1 in reference [2].  Instead of cross-entropy loss on the last line, instead the computed loss is the KL divergence between the current distribution and the target distribution, as in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_Update(policy_net, target_net, experiences, device):\n",
    "    \n",
    "    distribution_hyperparameters = policy_net.distribution_hyperparameters\n",
    "    \n",
    "    # Convert experiences to tensors, short alias \n",
    "    # hyperparameters since somewhat intense here\n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences[0]])).to(device)\n",
    "    actions = torch.tensor([e.data.action for e in experiences[0]]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences[0]]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.tensor(np.array([e.data.last_state for e in experiences[0]])).to(device)\n",
    "    \n",
    "    p_i_total = experiences[1][2]\n",
    "    # Normalized p_i see eqn (1) of reference [3]\n",
    "    p_i = torch.tensor([(e.p_i/p_i_total) for e in experiences[0]]).to(device) \n",
    "    \n",
    "    buf_N = torch.tensor(experiences[1][0]).to(device)\n",
    "    beta = torch.tensor(experiences[1][1]).to(device)\n",
    "    gamma = torch.tensor([distribution_hyperparameters.gamma]).to(device)\n",
    "    n_step_order = torch.tensor([distribution_hyperparameters.n_step_order]).to(device)\n",
    "    v_min = torch.tensor([distribution_hyperparameters.v_min]).to(device)\n",
    "    v_max = torch.tensor([distribution_hyperparameters.v_max]).to(device)\n",
    "    z_i = distribution_hyperparameters.z_i.to(device)\n",
    "    n_atoms = policy_net.n_atoms\n",
    "    delta_z = (v_max - v_min) / (n_atoms - 1)\n",
    "    num_exp = len(init_states)\n",
    "    num_act = policy_net.num_actions\n",
    "    \n",
    "    # Input checks\n",
    "    tensor_check(p_i,'p_i')\n",
    "    \n",
    "    # Get d_t\n",
    "    d_t_all_actions = policy_net(init_states)\n",
    "    d_t = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t = torch.vstack((d_t,d_t_all_actions[i,(actions[i]*n_atoms):((actions[i]+1)*n_atoms)]))\n",
    "    tensor_check(d_t,'d_t')\n",
    "    \n",
    "    # Get d_t_prime\n",
    "    d_t_prime_all_actions = target_net(final_states)\n",
    "    mean_action_values = torch.empty((num_exp,0),requires_grad=True).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = \\\n",
    "            torch.hstack((mean_action_values,\n",
    "                torch.matmul(d_t_prime_all_actions[:,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    targ_act = torch.argmax(mean_action_values,dim=1)\n",
    "    d_t_prime = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t_prime = torch.vstack( \\\n",
    "            (d_t_prime,\n",
    "             d_t_prime_all_actions[i,(targ_act[i]*n_atoms):((targ_act[i]+1)*n_atoms)]))\n",
    "    tensor_check(d_t_prime,'d_t_prime')\n",
    "        \n",
    "    # Compute target distribution m_i.  See algorithm 1 of reference [2]\n",
    "    m_i = torch.zeros((num_exp,n_atoms)).to(device)\n",
    "    disc = torch.pow(gamma,n_step_order).to(device)\n",
    "    Tz_j = torch.clamp(torch.add(rewards,torch.mul(disc,d_t_prime)), v_min, v_max)\n",
    "    b_j = torch.div(torch.sub(Tz_j,v_min),delta_z)\n",
    "    l_bins = torch.floor(b_j).long()\n",
    "    u_bins = torch.ceil(b_j).long()\n",
    "    tensor_check(Tz_j,'Tz_j')\n",
    "    tensor_check(b_j,'b_j')\n",
    "    for j in range(n_atoms):\n",
    "        test_rslt = torch.sub(u_bins[:,j],b_j[:,j])\n",
    "        m_i[torch.arange(num_exp).long(),l_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),l_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(u_bins[:,j],b_j[:,j])))\n",
    "        m_i[torch.arange(num_exp).long(),u_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),u_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(b_j[:,j],l_bins[:,j])))\n",
    "    \n",
    "    # Compute KL divergence between distributions for each experience\n",
    "    # Return loss for all experiences in batch.  We do not reduce to mean\n",
    "    # because the individual losses are needed to update the prioritized\n",
    "    # experience replay buffer.\n",
    "    biased_loss = F.kl_div(d_t.log(),m_i,reduction='none')\n",
    "    tensor_check(biased_loss,'biased_loss')\n",
    "    \n",
    "    # Multiply losses by importance sampling weights to correct for distribution change\n",
    "    # with prioritized sampling.  Clamp p_i because reciprocal function is scary\n",
    "    p_i = torch.clamp(p_i,min=1e-9)\n",
    "    w_i = torch.pow( \\\n",
    "            torch.reciprocal( \\\n",
    "              torch.mul(buf_N,p_i)),beta).unsqueeze(dim=1)\n",
    "    w_i = torch.clamp(w_i,1e-8,1.0)\n",
    "    tensor_check(w_i,'w_i')\n",
    "    return (torch.mul(w_i,biased_loss), w_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729af8",
   "metadata": {},
   "source": [
    "#### Training Session Options\n",
    "`max_episodes`: Maximum number of episodes before stopping training run<br>\n",
    "`rewards_buffer`: Pass in empty list or existing list.  Average rewards per episode data is appended to this list<br>\n",
    "`reward_window`: Length of averaging window for reporting rewards, in number of episodes<br>\n",
    "`report_interval`: Interval in episodes for updating `rewards_buffer` and displaying status<br>\n",
    "`solved_threshold`: Average reward over averaging window required to stop training early<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93602f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the episodic scores and running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_window_size=100,starting_scores=None):\n",
    "        self.avg_window_size = avg_window_size\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_window_size\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_window_size:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_window_size + 1)] / self.avg_window_size\n",
    "            \n",
    "    def run_avg(self):\n",
    "        if len(self.scores) < self.avg_window_size:\n",
    "            return None\n",
    "        else:\n",
    "            return self.internal_run_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test PerformanceLogger\n",
    "test_perf_log = PerformanceLogger()\n",
    "for i in range(100):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'First average: {test_perf_log.run_avg()}')\n",
    "for i in range(100,200):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'Second average: {test_perf_log.run_avg()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOptions():\n",
    "    def __init__(self,max_episodes=2000,\n",
    "                 rewards_buffer=[],reward_window=100,\n",
    "                 report_interval=5,solved_threshold=13):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.rewards_buffer = rewards_buffer\n",
    "        self.report_interval = report_interval\n",
    "        self.solved_threshold = solved_threshold\n",
    "        self.performance_logger = PerformanceLogger(reward_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2728d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular buffer for the n-step rewards.  \n",
    "# Could be more efficient but n is only 3 \n",
    "# in methods of rainbow dqn paper and it worked\n",
    "# out for them.\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,n_step_order,gamma=1.0):\n",
    "        self.n_step_order = n_step_order\n",
    "        self.store = deque(maxlen = n_step_order)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == self.n_step_order\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state),p_i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdca606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is copied from the Lunar Lander dqn_agent.py file.\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects action number that has the highest expected return\n",
    "def sel_action(policy_net, state, device):\n",
    "    z_i = policy_net.distribution_hyperparameters.z_i.to(device)\n",
    "    num_act = policy_net.environment_hyperparameters.num_actions\n",
    "    n_atoms = policy_net.distribution_hyperparameters.n_atoms\n",
    "    \n",
    "    d_t = policy_net(torch.tensor(state).unsqueeze(dim=0).to(device))\n",
    "    mean_action_values = torch.tensor([],requires_grad=False).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = torch.hstack( \\\n",
    "                                (mean_action_values,\n",
    "                                 torch.matmul(d_t[0,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    return torch.argmax(mean_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73fa6",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`net`: Parameterized model to use for training run<br>\n",
    "`replay_buffer`: Replay buffer object to use for training run<br>\n",
    "`optimization_hyperparameters`: `OptimizationHyperparameters` object to use for training run<br>\n",
    "`train_options`: `TrainingOptions` object to use for training run\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to the `PerformanceLogger` of the supplied `train_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee256af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, \n",
    "              env,\n",
    "              opt_hyp=OptimizationHyperparameters(),\n",
    "              replay_buffer=PrioritizedReplayBuffer(),\n",
    "              train_options=TrainingOptions(),\n",
    "              ):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move model to device\n",
    "    policy_net = policy_net.to(device)\n",
    "    \n",
    "    # Aliases for readability only\n",
    "    perf_log = train_options.performance_logger\n",
    "    max_eps = train_options.max_episodes\n",
    "    n_step_order = policy_net.distribution_hyperparameters.n_step_order\n",
    "    gamma = policy_net.distribution_hyperparameters.gamma\n",
    "    omega = replay_buffer.omega\n",
    "    batch_size = opt_hyp.batch_size\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = Adam(policy_net.parameters(), lr=opt_hyp.lr, weight_decay=5e-4, eps = 1.5e-4)\n",
    "    \n",
    "    # Generate target network\n",
    "    target_net = ParameterizedModel(policy_net.environment_hyperparameters,\n",
    "                                    policy_net.model_hyperparameters,\n",
    "                                    policy_net.distribution_hyperparameters).to(device)\n",
    "    \n",
    "    # Counters\n",
    "    learn_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Get brain info for environment.  That is the term\n",
    "    # Unity ML-Agents for controllers of agents in\n",
    "    # simulation\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # Let's go\n",
    "    for episode in range(max_eps):\n",
    "        \n",
    "        n_step_buf = MultistepBuffer(n_step_order, gamma)\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            \n",
    "            # Environment interaction\n",
    "            action = sel_action(policy_net, state, device)\n",
    "            env_info = env.step(int(action.cpu()))[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # Process interaction results\n",
    "            score += reward\n",
    "            if not (reward == 0):\n",
    "                print(f'Episode {episode+1}: score = {score}', end='\\r')\n",
    "            n_step_buf.add_experience(Experience(state,action,reward,None))\n",
    "            \n",
    "            # If the multi-step buffer has enough experiences to make\n",
    "            # a full sequence, get the sequence, compute the loss on\n",
    "            # the sequence, add the experience with loss to the replay\n",
    "            # buffer\n",
    "            if n_step_buf.ready():\n",
    "                n_step_exp = n_step_buf.get_n_step_experience()\n",
    "                loss, w_i = Bellman_Update(policy_net, target_net, ([n_step_exp],(1,1,1)), device)\n",
    "                loss_py = loss.sum().item()\n",
    "                replay_buffer.add_experience(n_step_exp.data,loss[0].sum().item())\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "            learn_cntr += 1\n",
    "            if (learn_cntr % opt_hyp.learn_interval) == 0:\n",
    "                learn_cntr = 0\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    samp_exp = replay_buffer.sample(batch_size)\n",
    "                    \n",
    "                    # Zero gradients, get Bellman_Update loss, take mean, accumulate\n",
    "                    # gradients backward and step optimizer.\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_vec, w_i = \\\n",
    "                        Bellman_Update(policy_net, target_net, samp_exp, device)\n",
    "                    loss_vec = loss_vec.sum(dim=1)\n",
    "                    loss_avg = torch.div(torch.sum(loss_vec), batch_size)\n",
    "                    loss_avg.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                    # Update priorities in replay buffer according to losses\n",
    "                    new_p_i = torch.div(loss_vec,w_i.squeeze(dim=1)).pow(omega)\n",
    "                    for exp_num in range(batch_size):\n",
    "                            samp_exp[0][exp_num].update_p( \\\n",
    "                                (new_p_i[exp_num] - samp_exp[0][exp_num].p_i).item())\n",
    "                    \n",
    "                    # Soft update target network with policy network\n",
    "                    soft_update(policy_net, target_net, opt_hyp.target_alpha)\n",
    "    \n",
    "        perf_log.add_score(score)\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % train_options.report_interval) == 0:\n",
    "            rpt_cntr = 0\n",
    "            report = f'Completed {episode + 1} episodes. Average score = '\n",
    "            avg_score = perf_log.run_avg()\n",
    "            if len(perf_log.scores) < perf_log.avg_window_size:\n",
    "                avg_score = sum(perf_log.scores) / len(perf_log.scores)\n",
    "            print(f'{report}{avg_score:.2f}')\n",
    "        \n",
    "        if ((episode >= 99) and (perf_log.run_avg() >= train_options.solved_threshold)):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break    \n",
    "        \n",
    "        if episode == (max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {max_eps} episodes')\n",
    "            break\n",
    "            \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35241",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150edb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParameterizedModel()\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(model,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model,\"end_training_one.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n",
    "[5] Fortunato et. al., Noisy Networks for Exploration, arXiv:1706.10295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
