{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/bannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "  - Value Distribution\n",
    "  - Parameterized Model\n",
    "  - Prioritized Replay\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameters\n",
    "  - Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports and references\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace, and identify the path to the simulated environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from collections import namedtuple, deque\n",
    "from math import isnan\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Set to the path to simulated environment executable on system.\n",
    "env_location = \\\n",
    "\"C:\\Projects\\Value-based-methods\\p1_navigation\\Banana_Windows_x86_64\\Banana_Windows_x86_64\\Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $s$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "<BR>\n",
    "\n",
    "The neural network will be a fully connected MLP with one input for each state variable, at least one hidden layer, and one output for each pair $(z_i, a)$.  See the implentation section for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47142b86",
   "metadata": {},
   "source": [
    "### Prioritized Replay\n",
    "The algorithm will periodically switch between exploration and learning phases.  <br><br>During exploration phases, state transition tuples $(S_t,a_t,r_t,S_{t+1})$ will be collected, transformed to *n-step* transition events via an accumulation buffer, and stored in a prioritized experience buffer. \n",
    "<br><br>\n",
    "During learning phases, transition events sampled from the prioritized experience buffer will be used to optimize the parameterized model.  Like in [1], the probability of utilizing a transition $T$ from the experience buffer is consistent with the proportionality relation: <br><br>\n",
    "$$p_T \\varpropto (Loss)^{\\omega}, \\omega \\in [0,\\infty)$$\n",
    "<br>\n",
    "The hyperparameter $\\omega$ allows tuning of the degree to which the probability of selection is affected by loss magnitude [3].\n",
    "<br><br>Qualitatively, the $Loss$ in this context is proportional to how inconsistent the parameterized model's prediction is with a prediction that uses actual rewards sampled from the environment.  See the implementation section for detail on how the loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba884e3f",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "`state_dimension`: Dimension of the observable state space<br>\n",
    "`num_actions`: Number of actions available to agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5746ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentHyperparameters():\n",
    "    def __init__(self,state_dimension=37,num_actions=4):\n",
    "        self.state_dimension = state_dimension\n",
    "        self.num_actions = num_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43249fb8",
   "metadata": {},
   "source": [
    "#### Parameterized Model\n",
    "`num_hidden_layers`: How many hidden layers are included in the neural network model<br>\n",
    "`hidden_layer_size`: How many neurons are included in each hidden layer<br>\n",
    "`noise_init`: Initial standard deviation for all noise parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f239be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHyperparameters():\n",
    "    def __init__(self,num_hidden_layers=2,hidden_layer_size=300,noise_init=0.1):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.noise_init = noise_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c93654",
   "metadata": {},
   "source": [
    "#### Reward Distribution\n",
    "`v_min`: Lower limit clipping value for *n-step* return<br>\n",
    "`v_max`: Upper limit clipping value for *n-step* return<br>\n",
    "`gamma`: Discount factor for reward calculation.  An expected reward n steps in the future is discounted by $\\gamma^n$<br>\n",
    "`n_step_order`: How many environment steps from initial state are considered in constructing action value distribution<br>\n",
    "`n_atoms`: Number of discretization bins for representing possible value distribution returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e717b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionHyperparameters():\n",
    "    def __init__(self,v_min=-10,v_max=10,gamma=0.94,n_step_order=20,n_atoms=51):\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        self.gamma = gamma\n",
    "        self.n_step_order = n_step_order\n",
    "        self.n_atoms = n_atoms\n",
    "        self.z_i = \\\n",
    "            torch.tensor([(v_min + (i * ((v_max-v_min) / (n_atoms - 1)))) for i in range(n_atoms)])\n",
    "        self.z_i = torch.unsqueeze(self.z_i,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782dd24",
   "metadata": {},
   "source": [
    "#### Replay Buffer\n",
    "`buffer_size`: Maximum number of transition events that may be stored in the replay buffer<br>\n",
    "`omega`: Loss influence factor for transition event selection probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4744be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBufferHyperparameters():\n",
    "    def __init__(self,buffer_size=1e5,omega=0.4):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.omega = omega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8dff3c",
   "metadata": {},
   "source": [
    "#### Optimization Behavior\n",
    "`lr`: Learning rate for parameterized model optimizer<br>\n",
    "`learn_interval`: Number of environment exploration steps between each learning phase<br>\n",
    "`batch_size`: Number of n-step transition events to process during each learning phase <br>\n",
    "`target_alpha`: Soft update factor used so that target reward distribution lags policy reward distribution.  See [this explanation](https://deeplizard.com/learn/video/xVkPh9E9GfE) for why this is necessary.  The following update will be applied once per batch:<br><br>\n",
    "$$\\theta_{target} = (1-\\alpha)\\theta_{target} + \\alpha\\theta_{policy}$$ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb02b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationHyperparameters():\n",
    "    def __init__(self,lr=0.0000625,learn_interval=1,batch_size=64,target_alpha=5e-4):\n",
    "        self.lr = lr\n",
    "        self.learn_interval = learn_interval\n",
    "        self.batch_size = batch_size\n",
    "        self.target_alpha = target_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484595f5",
   "metadata": {},
   "source": [
    "Like in [3], a sum tree structure is used for efficient weighted sampling over a large number of items.  This implementation is heavily based on that in reference [4](http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/), but with some organizational changes to accomodate dynamic resizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that represents an experience in the experience buffer and/or a non-leaf node of the sum tree\n",
    "# Experience tuples are stored in the self.data attribute\n",
    "class SumTreeNode():\n",
    "    \n",
    "    def __init__(self,data=None,p_i=0):\n",
    "        self.data = data\n",
    "        self.p_i = p_i\n",
    "        self.parent = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "    \n",
    "    def update_p(self, delta_p):\n",
    "        self.p_i += delta_p\n",
    "        if self.parent is not None:\n",
    "            self.parent.update_p(delta_p)\n",
    "    \n",
    "    def attach_child(self,child):\n",
    "        if self.data is None:    # Not a leaf node\n",
    "            if self.left_child is None:    # No children, become leaf with cloned data\n",
    "                self.data = child.data\n",
    "                self.update_p(child.p_i - self.p_i)\n",
    "            else:    # Non-leaf node, attach to lower p_i side\n",
    "                if self.left_child.p_i < self.right_child.p_i:\n",
    "                    delegate_node = self.left_child\n",
    "                else:\n",
    "                    delegate_node = self.right_child\n",
    "                delegate_node.attach_child(child)\n",
    "        else:    # self is a leaf-node.  Clone self.data into new child, become non-leaf\n",
    "            self.left_child = SumTreeNode(self.data,self.p_i)\n",
    "            self.data = None\n",
    "            self.right_child = child\n",
    "            self.left_child.parent, self.right_child.parent = self, self     \n",
    "            self.update_p((self.left_child.p_i + self.right_child.p_i)- self.p_i)\n",
    "    \n",
    "    def remove(self):\n",
    "        if self.parent is not None:  # remove only has effect if there is a parent\n",
    "            if self.parent.left_child is self:\n",
    "                sibling = self.parent.right_child\n",
    "            else:\n",
    "                sibling = self.parent.left_child\n",
    "            # Clone data to parent from sibling and update references\n",
    "            if sibling is None:\n",
    "                print(f'Removing:{self.data} |||| {self.p_i}')\n",
    "                print(f'Parent:{self.parent.data} |||| {self.parent.p_i}')\n",
    "                print(f'Sibling:{sibling.data} |||| {sibling.p_i}')\n",
    "            self.parent.data = sibling.data\n",
    "            self.parent.left_child = sibling.left_child\n",
    "            self.parent.right_child = sibling.right_child\n",
    "            if (self.parent.left_child is not None):\n",
    "                self.parent.left_child.parent = self.parent\n",
    "            if (self.parent.right_child is not None):\n",
    "                self.parent.right_child.parent = self.parent\n",
    "            self.parent.update_p(sibling.p_i - self.parent.p_i)\n",
    "    \n",
    "    def weighted_retrieve(self,p_samp):\n",
    "        if self.data is not None: # must be a leaf-node\n",
    "            return self\n",
    "        else:\n",
    "            if self.left_child.p_i >= p_samp:\n",
    "                return self.left_child.weighted_retrieve(p_samp)\n",
    "            else:\n",
    "                return self.right_child.weighted_retrieve(p_samp - self.left_child.p_i)\n",
    "            \n",
    "    # This is used to identify low sampling probability nodes for removal\n",
    "    def minimal_node(self):\n",
    "        if self.data is not None:\n",
    "            return self\n",
    "        else:\n",
    "            min_left = self.left_child.minimal_node()\n",
    "            min_right = self.right_child.minimal_node()\n",
    "            return min_left if min_left.p_i < min_right.p_i else min_right  \n",
    "       \n",
    "    # Debug function\n",
    "    def check_node(self):\n",
    "        if self.data is not None:\n",
    "            return \"\"\n",
    "        if ((self.left_child is None) or (self.right_child is None)):\n",
    "            return f'Error: [{self.data}||{self.p_i}]'\n",
    "        left_check = self.left_child.check_node()\n",
    "        right_check = self.right_child.check_node()\n",
    "        if ((len(left_check) > 0) or (len(right_check) > 0)):\n",
    "            return f'[{self.data}||{self.p_i}]-->{self.left_child.check_node()}{self.right_child.check_node()}'\n",
    "        return \"\"\n",
    "    \n",
    "    # String representation of tree\n",
    "    def tree_str(self,lvl=0):\n",
    "        lvl_str= '--> '*lvl\n",
    "        left_str = self.left_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        right_str = self.right_child.tree_str(lvl+1) if self.left_child is not None else ''\n",
    "        par_str = self.parent.p_i if self.parent is not None else 'nothing'\n",
    "        my_str = f'{lvl_str}[{self.data}|{self.p_i}] under [{par_str}]\\n'\n",
    "        return f'{my_str}{left_str}{right_str}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95526535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized replay buffer definition\n",
    "\n",
    "# Experience aggregate\n",
    "Experience = namedtuple('Experience',['state','action','reward','last_state'])\n",
    "\n",
    "class PrioritizedReplayBuffer():\n",
    "    \n",
    "    def __init__(self,replay_buffer_hyperparameters=ReplayBufferHyperparameters()):\n",
    "        self.max_size = replay_buffer_hyperparameters.buffer_size\n",
    "        self.omega = replay_buffer_hyperparameters.omega\n",
    "        self.store = SumTreeNode()\n",
    "        self.current_size = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.current_size \n",
    "        \n",
    "    def add_experience(self, experience, loss):\n",
    "        if self.current_size >= self.max_size:    # Remove minimum p_i experience\n",
    "            min_node = self.store.minimal_node()\n",
    "            self.store.minimal_node().remove()\n",
    "        self.store.attach_child(SumTreeNode(experience, pow(loss,self.omega)))\n",
    "        if self.current_size < self.max_size:\n",
    "            self.current_size += 1\n",
    "            \n",
    "    def sample(self,batch_size):\n",
    "        if batch_size > self.current_size:\n",
    "            return None\n",
    "        sample_keys = (np.random.rand(batch_size)*self.store.p_i).tolist()\n",
    "        return [self.store.weighted_retrieve(p_samp) for p_samp in sample_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "068dc3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Counts\n",
      "one: 0.0\n",
      "two: 2.0448979591836736\n",
      "three: 2.836734693877551\n",
      "four: 1.0\n",
      "five: 1.8775510204081634\n",
      "six: 2.983673469387755\n",
      "seven: 0.9714285714285714\n",
      "eight: 1.9061224489795918\n",
      "nine: 2.706122448979592\n"
     ]
    }
   ],
   "source": [
    "# Unit test for PrioritizedReplayBuffer\n",
    "test_rep_hyp = ReplayBufferHyperparameters(buffer_size = 8, omega = 1)\n",
    "test_rep_buffer = PrioritizedReplayBuffer(test_rep_hyp)\n",
    "test_experiences = ['one','two','three','four','five','six','seven','eight','nine']\n",
    "test_weights = [0.99,2,3,1,2,3,1,2,3]\n",
    "test_batch_size = 4\n",
    "test_batches = 1000\n",
    "for (e,wt) in zip(test_experiences,test_weights):\n",
    "    test_rep_buffer.add_experience(e,wt)\n",
    "test_samples = []\n",
    "for i in range(test_batches):\n",
    "    test_samples += [sample.data for sample in test_rep_buffer.sample(test_batch_size)]\n",
    "print(\"Normalized Counts\")\n",
    "for exp in test_experiences:\n",
    "    print(f'{exp}: {test_samples.count(exp) / test_samples.count(\"four\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3e6e6",
   "metadata": {},
   "source": [
    "The linear layers have noisy bias and weight components, implemented with factored gaussian noise according to [5].  The standard deviation of each noise component is subject to optimization (learning) by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,noise_init=0.05):\n",
    "        super(NoisyLinear,self).__init__()\n",
    "        self.deterministic_linear = nn.Linear(in_dim,out_dim)\n",
    "        self.noisy_weights = \\\n",
    "            nn.Parameter(noise_init*torch.ones(in_dim,out_dim,dtype=torch.float32))\n",
    "        self.noisy_bias = nn.Parameter(noise_init*torch.ones(1,out_dim,dtype=torch.float32))\n",
    "    \n",
    "    def noise_transform(self,x):\n",
    "        # See section 3(b) of reference [5]\n",
    "        return torch.mul(torch.sgn(x),torch.sqrt(torch.abs(x)))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # Generate factorized gaussian noise, clamping to +/- 3 sigma\n",
    "        in_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((self.noisy_weights.size(dim=0),1),device=x.device),-3.0,3.0))\n",
    "        out_dim_noise = \\\n",
    "            self.noise_transform( \\\n",
    "                torch.clamp( \\\n",
    "                    torch.randn((1,self.noisy_weights.size(dim=1)),device=x.device),-3.0,3.0))\n",
    "        \n",
    "        weight_noise = torch.mul(self.noisy_weights,torch.matmul(in_dim_noise,out_dim_noise))\n",
    "        bias_noise = torch.mul(self.noisy_bias,out_dim_noise)\n",
    "    \n",
    "        x = x.float()\n",
    "        return self.deterministic_linear(x) + torch.clamp(torch.matmul(x,weight_noise) + bias_noise,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319ce3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:torch.Size([5, 10])\n",
      "Output size:torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Unit test for NoisyLinear\n",
    "test_noisy_layer = NoisyLinear(10,4)\n",
    "test_input_data = torch.ones((5,10))\n",
    "test_output = test_noisy_layer(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f76476cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterized Model Implementation\n",
    "class ParameterizedModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 environment_hyperparameters = EnvironmentHyperparameters(),\n",
    "                 model_hyperparameters = ModelHyperparameters(),\n",
    "                 distribution_hyperparameters = DistributionHyperparameters()\n",
    "                ):\n",
    "        super(ParameterizedModel,self).__init__()\n",
    "        in_dim = environment_hyperparameters.state_dimension\n",
    "        out_dim = environment_hyperparameters.num_actions \\\n",
    "                  * distribution_hyperparameters.n_atoms\n",
    "        hidden_size = model_hyperparameters.hidden_layer_size\n",
    "        num_hidden_layers = model_hyperparameters.num_hidden_layers\n",
    "        noise_init = model_hyperparameters.noise_init\n",
    "        \n",
    "        self.distribution_hyperparameters = distribution_hyperparameters\n",
    "        self.environment_hyperparameters = environment_hyperparameters\n",
    "        self.model_hyperparameters = model_hyperparameters\n",
    "        self.num_actions = environment_hyperparameters.num_actions\n",
    "        self.n_atoms = distribution_hyperparameters.n_atoms\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(NoisyLinear(in_dim,hidden_size,noise_init))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(num_hidden_layers-1):\n",
    "            layers.append(NoisyLinear(hidden_size,hidden_size,noise_init))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(NoisyLinear(hidden_size,out_dim,noise_init))\n",
    "        \n",
    "        self.reg_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.reg_layers(x)\n",
    "        \n",
    "        # See reference [1], softmax must be calculated individually on the\n",
    "        # atoms for each action since each action is a probability distribution\n",
    "        y = torch.tensor([],requires_grad=True,device=x.device)\n",
    "        for i in range(self.num_actions):\n",
    "            y = torch.hstack((y,F.softmax(x[:,(i*self.n_atoms):((i+1)*self.n_atoms)],dim=1)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4908a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size:torch.Size([5, 11])\n",
      "Output size:torch.Size([5, 21])\n"
     ]
    }
   ],
   "source": [
    "# Unit test for ParameterizedModel\n",
    "test_env_hyp = EnvironmentHyperparameters(11,3)\n",
    "test_mdl_hyp = ModelHyperparameters(3,30)\n",
    "test_dst_hyp = DistributionHyperparameters(n_atoms = 7)\n",
    "test_mdl = ParameterizedModel(test_env_hyp, test_mdl_hyp, test_dst_hyp)\n",
    "test_input_data = torch.randn(5,11)\n",
    "test_output = test_mdl(test_input_data)\n",
    "print(f'Input size:{test_input_data.size()}')\n",
    "print(f'Output size:{test_output.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ca8356",
   "metadata": {},
   "source": [
    "See algorithm 1 in reference [2].  Instead of cross-entropy loss on the last line, instead the computed loss is the KL divergence between the current distribution and the target distribution, as in [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_Update(policy_net, target_net, experiences, device):\n",
    "    \n",
    "    distribution_hyperparameters = policy_net.distribution_hyperparameters\n",
    "    \n",
    "    # Convert experiences to tensors, short alias \n",
    "    # hyperparameters since somewhat intense here\n",
    "    init_states = torch.tensor(np.array([e.data.state for e in experiences])).to(device)\n",
    "    actions = torch.tensor([e.data.action for e in experiences]).to(device)\n",
    "    rewards = torch.tensor([e.data.reward for e in experiences]).unsqueeze(dim=1).to(device)\n",
    "    final_states = torch.tensor(np.array([e.data.last_state for e in experiences])).to(device)\n",
    "    gamma = torch.tensor([distribution_hyperparameters.gamma]).to(device)\n",
    "    n_step_order = torch.tensor([distribution_hyperparameters.n_step_order]).to(device)\n",
    "    v_min = torch.tensor([distribution_hyperparameters.v_min]).to(device)\n",
    "    v_max = torch.tensor([distribution_hyperparameters.v_max]).to(device)\n",
    "    z_i = distribution_hyperparameters.z_i.to(device)\n",
    "    n_atoms = policy_net.n_atoms\n",
    "    delta_z = (v_max - v_min) / (n_atoms - 1)\n",
    "    num_exp = len(init_states)\n",
    "    num_act = policy_net.num_actions\n",
    "    \n",
    "    # Get d_t\n",
    "    d_t_all_actions = policy_net(init_states)\n",
    "    d_t = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t = torch.vstack((d_t,d_t_all_actions[i,(actions[i]*n_atoms):((actions[i]+1)*n_atoms)]))\n",
    "    \n",
    "    # Get d_t_prime\n",
    "    d_t_prime_all_actions = target_net(final_states)\n",
    "    mean_action_values = torch.empty((num_exp,0),requires_grad=True).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = \\\n",
    "            torch.hstack((mean_action_values,\n",
    "                torch.matmul(d_t_prime_all_actions[:,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    targ_act = torch.argmax(mean_action_values,dim=1)\n",
    "    d_t_prime = torch.empty((0,n_atoms),requires_grad=True).to(device)\n",
    "    for i in range(num_exp):\n",
    "        d_t_prime = torch.vstack( \\\n",
    "            (d_t_prime,\n",
    "             d_t_prime_all_actions[i,(targ_act[i]*n_atoms):((targ_act[i]+1)*n_atoms)]))\n",
    "        \n",
    "    # Compute target distribution m_i.  See algorithm 1 of reference [2]\n",
    "    m_i = torch.zeros((num_exp,n_atoms)).to(device)\n",
    "    disc = torch.pow(gamma,n_step_order).to(device)\n",
    "    Tz_j = torch.clamp(torch.add(rewards,torch.mul(disc,d_t_prime)), v_min, v_max)\n",
    "    b_j = torch.div(torch.sub(Tz_j,v_min),delta_z)\n",
    "    l_bins = torch.floor(b_j).long()\n",
    "    u_bins = torch.ceil(b_j).long()\n",
    "    for j in range(n_atoms):\n",
    "        test_rslt = torch.sub(u_bins[:,j],b_j[:,j])\n",
    "        m_i[torch.arange(num_exp).long(),l_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),l_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(u_bins[:,j],b_j[:,j])))\n",
    "        m_i[torch.arange(num_exp).long(),u_bins[:,j]] = \\\n",
    "            torch.add(m_i[torch.arange(num_exp).long(),u_bins[:,j]],\n",
    "                      torch.mul(d_t_prime[:,j],torch.sub(b_j[:,j],l_bins[:,j])))\n",
    "    \n",
    "    # Compute KL divergence between distributions for each experience\n",
    "    # Return loss for all experiences in batch.  We do not reduce to mean\n",
    "    # because the individual losses are needed to update the prioritized\n",
    "    # experience replay buffer.\n",
    "    return F.kl_div(d_t.log(),m_i,reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729af8",
   "metadata": {},
   "source": [
    "#### Training Session Options\n",
    "`max_episodes`: Maximum number of episodes before stopping training run<br>\n",
    "`rewards_buffer`: Pass in empty list or existing list.  Average rewards per episode data is appended to this list<br>\n",
    "`reward_window`: Length of averaging window for reporting rewards, in number of episodes<br>\n",
    "`report_interval`: Interval in episodes for updating `rewards_buffer` and displaying status<br>\n",
    "`solved_threshold`: Average reward over averaging window required to stop training early<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93602f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger for the episodic scores and running average\n",
    "class PerformanceLogger():\n",
    "    def __init__(self,avg_window_size=100,starting_scores=None):\n",
    "        self.avg_window_size = avg_window_size\n",
    "        self.scores = starting_scores if starting_scores is not None else []\n",
    "        self.internal_run_avg = 0\n",
    "        \n",
    "    def add_score(self,score):\n",
    "        self.scores.append(score)\n",
    "        self.internal_run_avg += score / self.avg_window_size\n",
    "        # Remove tail of running average\n",
    "        if len(self.scores) > self.avg_window_size:\n",
    "            self.internal_run_avg -= self.scores[-(self.avg_window_size + 1)] / self.avg_window_size\n",
    "            \n",
    "    def run_avg(self):\n",
    "        if len(self.scores) < self.avg_window_size:\n",
    "            return None\n",
    "        else:\n",
    "            return self.internal_run_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a07bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First average: 49.5\n",
      "Second average: 149.5\n"
     ]
    }
   ],
   "source": [
    "# Unit test PerformanceLogger\n",
    "test_perf_log = PerformanceLogger()\n",
    "for i in range(100):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'First average: {test_perf_log.run_avg()}')\n",
    "for i in range(100,200):\n",
    "    test_perf_log.add_score(i)\n",
    "print(f'Second average: {test_perf_log.run_avg()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86e4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingOptions():\n",
    "    def __init__(self,max_episodes=2000,\n",
    "                 rewards_buffer=[],reward_window=100,\n",
    "                 report_interval=5,solved_threshold=13):\n",
    "        self.max_episodes = max_episodes\n",
    "        self.rewards_buffer = rewards_buffer\n",
    "        self.report_interval = report_interval\n",
    "        self.solved_threshold = solved_threshold\n",
    "        self.performance_logger = PerformanceLogger(reward_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2728d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circular buffer for the n-step rewards.  \n",
    "# Could be more efficient but n is only 3 \n",
    "# in methods of rainbow dqn paper and it worked\n",
    "# out for them.\n",
    "class MultistepBuffer():\n",
    "    def __init__(self,n_step_order,gamma=1.0):\n",
    "        self.n_step_order = n_step_order\n",
    "        self.store = deque(maxlen = n_step_order)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def add_experience(self,exp):\n",
    "        self.store.append(exp)\n",
    "    \n",
    "    def ready(self):\n",
    "        return len(self.store) == self.n_step_order\n",
    "    \n",
    "    def get_n_step_experience(self):\n",
    "        out_state = self.store[0].state\n",
    "        out_action = self.store[0].action\n",
    "        out_reward = \\\n",
    "            sum([((self.store[i].reward) * pow(self.gamma,i)) for i in range(self.n_step_order)])\n",
    "        out_final_state = self.store[-1].state\n",
    "        return SumTreeNode(Experience(out_state, out_action, out_reward, out_final_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbdca606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is copied from the Lunar Lander dqn_agent.py file.\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aa9ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects action number that has the highest expected return\n",
    "def sel_action(policy_net, state, device):\n",
    "    z_i = policy_net.distribution_hyperparameters.z_i.to(device)\n",
    "    num_act = policy_net.environment_hyperparameters.num_actions\n",
    "    n_atoms = policy_net.distribution_hyperparameters.n_atoms\n",
    "    \n",
    "    d_t = policy_net(torch.tensor(state).unsqueeze(dim=0).to(device))\n",
    "    mean_action_values = torch.tensor([],requires_grad=False).to(device)\n",
    "    for i in range(num_act):\n",
    "        mean_action_values = torch.hstack( \\\n",
    "                                (mean_action_values,\n",
    "                                 torch.matmul(d_t[0,(i*n_atoms):((i+1)*n_atoms)],z_i)))\n",
    "    return torch.argmax(mean_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d73fa6",
   "metadata": {},
   "source": [
    "#### `train_net` Parameters\n",
    "`net`: Parameterized model to use for training run<br>\n",
    "`replay_buffer`: Replay buffer object to use for training run<br>\n",
    "`optimization_hyperparameters`: `OptimizationHyperparameters` object to use for training run<br>\n",
    "`train_options`: `TrainingOptions` object to use for training run\n",
    "\n",
    "#### `train_net` Returns\n",
    "Reference to the `PerformanceLogger` of the supplied `train_options`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ee256af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(policy_net, \n",
    "              env,\n",
    "              opt_hyp=OptimizationHyperparameters(),\n",
    "              replay_buffer=PrioritizedReplayBuffer(),\n",
    "              train_options=TrainingOptions(),\n",
    "              ):\n",
    "    \n",
    "    # Use gpu if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Move model to device\n",
    "    policy_net = policy_net.to(device)\n",
    "    \n",
    "    # Aliases for readability only\n",
    "    perf_log = train_options.performance_logger\n",
    "    max_eps = train_options.max_episodes\n",
    "    n_step_order = policy_net.distribution_hyperparameters.n_step_order\n",
    "    gamma = policy_net.distribution_hyperparameters.gamma\n",
    "    batch_size = opt_hyp.batch_size\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = Adam(policy_net.parameters(), lr=opt_hyp.lr, weight_decay=1e-3, eps = 1.5e-4)\n",
    "    \n",
    "    # Generate target network\n",
    "    target_net = ParameterizedModel(policy_net.environment_hyperparameters,\n",
    "                                    policy_net.model_hyperparameters,\n",
    "                                    policy_net.distribution_hyperparameters).to(device)\n",
    "    \n",
    "    # Counters\n",
    "    learn_cntr = 0\n",
    "    rpt_cntr = 0\n",
    "    \n",
    "    # Get brain info for environment.  That is the term\n",
    "    # Unity ML-Agents for controllers of agents in\n",
    "    # simulation\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "    \n",
    "    # Let's go\n",
    "    for episode in range(max_eps):\n",
    "        \n",
    "        n_step_buf = MultistepBuffer(n_step_order, gamma)\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        done = False\n",
    "        \n",
    "        while done == False:\n",
    "            \n",
    "            # Environment interaction\n",
    "            action = sel_action(policy_net, state, device)\n",
    "            env_info = env.step(int(action.cpu()))[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # Process interaction results\n",
    "            score += reward\n",
    "            if not (reward == 0):\n",
    "                print(f'Episode {episode+1}: score = {score}', end='\\r')\n",
    "            n_step_buf.add_experience(Experience(state,action,reward,None))\n",
    "            \n",
    "            # If the multi-step buffer has enough experiences to make\n",
    "            # a full sequence, get the sequence, compute the loss on\n",
    "            # the sequence, add the experience with loss to the replay\n",
    "            # buffer\n",
    "            if n_step_buf.ready():\n",
    "                n_step_exp = n_step_buf.get_n_step_experience()\n",
    "                loss = Bellman_Update(policy_net, target_net, [n_step_exp], device)\n",
    "                loss_py = loss.sum().item()\n",
    "                replay_buffer.add_experience(n_step_exp.data,loss.sum().item())\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "            learn_cntr += 1\n",
    "            if (learn_cntr % opt_hyp.learn_interval) == 0:\n",
    "                learn_cntr = 0\n",
    "                if len(replay_buffer) >= batch_size:\n",
    "                    samp_exp = replay_buffer.sample(batch_size)\n",
    "                    \n",
    "                    # Zero gradients, get Bellman_Update loss, take mean, accumulate\n",
    "                    # gradients backward and step optimizer.\n",
    "                    optimizer.zero_grad()\n",
    "                    loss_vec = \\\n",
    "                        Bellman_Update(policy_net, target_net, samp_exp, device).sum(dim=1)\n",
    "                    loss_avg = torch.div(torch.sum(loss_vec), batch_size)\n",
    "                    loss_avg.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                    # Update priorities in replay buffer according to losses\n",
    "                    for exp_num in range(batch_size):\n",
    "                            samp_exp[exp_num].update_p( \\\n",
    "                                (loss_vec[exp_num] - samp_exp[exp_num].p_i).item())\n",
    "                    \n",
    "                    # Soft update target network with policy network\n",
    "                    soft_update(policy_net, target_net, opt_hyp.target_alpha)\n",
    "    \n",
    "        perf_log.add_score(score)\n",
    "        \n",
    "        rpt_cntr += 1\n",
    "        if (rpt_cntr % train_options.report_interval) == 0:\n",
    "            rpt_cntr = 0\n",
    "            report = f'Completed {episode + 1} episodes. Average score = '\n",
    "            avg_score = perf_log.run_avg()\n",
    "            if len(perf_log.scores) < perf_log.avg_window_size:\n",
    "                avg_score = sum(perf_log.scores) / len(perf_log.scores)\n",
    "            print(f'{report}{avg_score:.2f}')\n",
    "        \n",
    "        if ((episode >= 99) and (perf_log.run_avg() >= train_options.solved_threshold)):\n",
    "            print(f'Solved with average score of {perf_log.run_avg()} in {episode+1} episodes')\n",
    "            break    \n",
    "        \n",
    "        if episode == (max_eps - 1):\n",
    "            print(f'Failed to solve within the maximum of {max_eps} episodes')\n",
    "            break\n",
    "            \n",
    "    return perf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35241",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "150edb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5 episodes. Average score = -0.80\n",
      "Completed 10 episodes. Average score = -0.40\n",
      "Completed 15 episodes. Average score = -0.33\n",
      "Completed 20 episodes. Average score = -0.10\n",
      "Completed 25 episodes. Average score = -0.08\n",
      "Completed 30 episodes. Average score = -0.10\n",
      "Completed 35 episodes. Average score = 0.00\n",
      "Completed 40 episodes. Average score = 0.07\n",
      "Completed 45 episodes. Average score = 0.11\n",
      "Completed 50 episodes. Average score = 0.40\n",
      "Completed 55 episodes. Average score = 0.80\n",
      "Completed 60 episodes. Average score = 1.13\n",
      "Completed 65 episodes. Average score = 1.71\n",
      "Completed 70 episodes. Average score = 1.79\n",
      "Completed 75 episodes. Average score = 2.12\n",
      "Completed 80 episodes. Average score = 2.46\n",
      "Completed 85 episodes. Average score = 2.62\n",
      "Completed 90 episodes. Average score = 2.62\n",
      "Completed 95 episodes. Average score = 2.82\n",
      "Completed 100 episodes. Average score = 3.13\n",
      "Completed 105 episodes. Average score = 3.58\n",
      "Completed 110 episodes. Average score = 3.96\n",
      "Completed 115 episodes. Average score = 4.43\n",
      "Completed 120 episodes. Average score = 4.78\n",
      "Completed 125 episodes. Average score = 5.26\n",
      "Completed 130 episodes. Average score = 5.69\n",
      "Completed 135 episodes. Average score = 6.13\n",
      "Completed 140 episodes. Average score = 6.32\n",
      "Completed 145 episodes. Average score = 6.63\n",
      "Completed 150 episodes. Average score = 6.85\n",
      "Completed 155 episodes. Average score = 7.04\n",
      "Completed 160 episodes. Average score = 7.44\n",
      "Completed 165 episodes. Average score = 7.46\n",
      "Completed 170 episodes. Average score = 7.63\n",
      "Completed 175 episodes. Average score = 7.65\n",
      "Completed 180 episodes. Average score = 7.80\n",
      "Completed 185 episodes. Average score = 7.89\n",
      "Completed 190 episodes. Average score = 8.35\n",
      "Completed 195 episodes. Average score = 8.45\n",
      "Completed 200 episodes. Average score = 8.47\n",
      "Completed 205 episodes. Average score = 8.43\n",
      "Completed 210 episodes. Average score = 8.32\n",
      "Completed 215 episodes. Average score = 8.58\n",
      "Completed 220 episodes. Average score = 8.61\n",
      "Completed 225 episodes. Average score = 8.76\n",
      "Completed 230 episodes. Average score = 8.55\n",
      "Completed 235 episodes. Average score = 8.54\n",
      "Completed 240 episodes. Average score = 8.69\n",
      "Completed 245 episodes. Average score = 8.93\n",
      "Completed 250 episodes. Average score = 8.88\n",
      "Completed 255 episodes. Average score = 8.87\n",
      "Completed 260 episodes. Average score = 8.77\n",
      "Completed 265 episodes. Average score = 8.77\n",
      "Completed 270 episodes. Average score = 8.88\n",
      "Completed 275 episodes. Average score = 8.72\n",
      "Completed 280 episodes. Average score = 8.55\n",
      "Completed 285 episodes. Average score = 8.60\n",
      "Completed 290 episodes. Average score = 8.51\n",
      "Completed 295 episodes. Average score = 8.80\n",
      "Completed 300 episodes. Average score = 8.77\n",
      "Completed 305 episodes. Average score = 8.75\n",
      "Completed 310 episodes. Average score = 8.95\n",
      "Completed 315 episodes. Average score = 8.57\n",
      "Completed 320 episodes. Average score = 8.60\n",
      "Completed 325 episodes. Average score = 8.44\n",
      "Completed 330 episodes. Average score = 8.57\n",
      "Completed 335 episodes. Average score = 8.43\n",
      "Completed 340 episodes. Average score = 8.57\n",
      "Completed 345 episodes. Average score = 8.60\n",
      "Completed 350 episodes. Average score = 8.81\n",
      "Completed 355 episodes. Average score = 8.93\n",
      "Completed 360 episodes. Average score = 8.87\n",
      "Completed 365 episodes. Average score = 8.89\n",
      "Completed 370 episodes. Average score = 8.77\n",
      "Completed 375 episodes. Average score = 9.09\n",
      "Completed 380 episodes. Average score = 9.37\n",
      "Completed 385 episodes. Average score = 9.45\n",
      "Completed 390 episodes. Average score = 9.41\n",
      "Completed 395 episodes. Average score = 9.18\n",
      "Completed 400 episodes. Average score = 9.18\n",
      "Completed 405 episodes. Average score = 9.44\n",
      "Completed 410 episodes. Average score = 9.38\n",
      "Completed 415 episodes. Average score = 9.67\n",
      "Completed 420 episodes. Average score = 9.65\n",
      "Completed 425 episodes. Average score = 9.47\n",
      "Completed 430 episodes. Average score = 9.60\n",
      "Completed 435 episodes. Average score = 9.81\n",
      "Completed 440 episodes. Average score = 9.75\n",
      "Completed 445 episodes. Average score = 9.60\n",
      "Completed 450 episodes. Average score = 9.43\n",
      "Completed 455 episodes. Average score = 9.37\n",
      "Completed 460 episodes. Average score = 9.26\n",
      "Completed 465 episodes. Average score = 9.24\n",
      "Completed 470 episodes. Average score = 9.36\n",
      "Completed 475 episodes. Average score = 9.47\n",
      "Completed 480 episodes. Average score = 9.50\n",
      "Completed 485 episodes. Average score = 9.48\n",
      "Completed 490 episodes. Average score = 9.57\n",
      "Completed 495 episodes. Average score = 9.55\n",
      "Completed 500 episodes. Average score = 9.66\n",
      "Completed 505 episodes. Average score = 9.50\n",
      "Completed 510 episodes. Average score = 9.70\n",
      "Completed 515 episodes. Average score = 9.54\n",
      "Completed 520 episodes. Average score = 9.57\n",
      "Completed 525 episodes. Average score = 9.72\n",
      "Completed 530 episodes. Average score = 9.79\n",
      "Completed 535 episodes. Average score = 9.98\n",
      "Completed 540 episodes. Average score = 10.04\n",
      "Completed 545 episodes. Average score = 10.28\n",
      "Completed 550 episodes. Average score = 10.38\n",
      "Completed 555 episodes. Average score = 10.15\n",
      "Completed 560 episodes. Average score = 10.16\n",
      "Completed 565 episodes. Average score = 10.12\n",
      "Completed 570 episodes. Average score = 10.46\n",
      "Completed 575 episodes. Average score = 10.34\n",
      "Completed 580 episodes. Average score = 10.21\n",
      "Completed 585 episodes. Average score = 10.13\n",
      "Completed 590 episodes. Average score = 9.97\n",
      "Completed 595 episodes. Average score = 10.04\n",
      "Completed 600 episodes. Average score = 9.89\n",
      "Completed 605 episodes. Average score = 9.82\n",
      "Completed 610 episodes. Average score = 9.61\n",
      "Episode 614: score = 5.00\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4ebd4d350619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameterizedModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrslt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-cbfde1cc08ec>\u001b[0m in \u001b[0;36mtrain_net\u001b[1;34m(policy_net, env, opt_hyp, replay_buffer, train_options)\u001b[0m\n\u001b[0;32m     81\u001b[0m                     \u001b[1;31m# Zero gradients, get Bellman_Update loss, take mean, accumulate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                     \u001b[1;31m# gradients backward and step optimizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                     \u001b[0mloss_vec\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                         \u001b[0mBellman_Update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamp_exp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_zero_grad_profile_name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hook_for_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zero_grad_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = ParameterizedModel()\n",
    "env = UnityEnvironment(file_name=env_location)\n",
    "rslt = train_net(model,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "torch.save(model,\"end_training_one.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rslt.scores)\n",
    "plt.title('Training Results')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952<br>\n",
    "[4] http://www.sefidian.com/2022/09/09/sumtree-data-structure-for-prioritized-experience-replay-per-explained-with-python-code/<br>\n",
    "[5] Fortunato et. al., Noisy Networks for Exploration, arXiv:1706.10295"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
