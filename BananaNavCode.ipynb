{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6e5c5a",
   "metadata": {},
   "source": [
    "# Banana Navigation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87150e01",
   "metadata": {},
   "source": [
    "![Screenshot of banana environment](doc/bannerImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936349ec",
   "metadata": {},
   "source": [
    "This is an implementation of Deep Reinforcement Q-Learning, applied to train an agent with four possible actions (move left, right, forward, or backward), to pick up yellow bananas and avoid blue bananas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d520f7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "+ Environment Setup\n",
    "+ Description of Algorithm\n",
    "+ Implementation of Algorithm\n",
    "  - Hyperparameter Definitions\n",
    "  - Multi-step Learning, Prioritized Replay Buffer\n",
    "  - Action Value Distribution Function (Neural Network)\n",
    "  - Bellman Update (Loss) Computation\n",
    "  - Training Loop\n",
    "+ Training\n",
    "+ Results\n",
    "+ References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e8df1",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241864bb",
   "metadata": {},
   "source": [
    "+ Follow instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to set up the environment, *with the following changes:*\n",
    "  - Before running `pip install .`, edit `Value-based-methods/python/requirements.txt` and remove the `torch==0.4.0` line\n",
    "  - After running `pip install .`, run the appropriate PyTorch installation command for your system indicated [here](https://pytorch.org/get-started/locally/)\n",
    "  - Continue following the instructions [here](https://github.com/udacity/Value-based-methods#dependencies) to their conclusion.\n",
    "+ Download the appropriate Unity Environment for your platform:\n",
    "  - [Linux](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux.zip)\n",
    "  - [Mac OSX](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana.app.zip)\n",
    "  - [Windows (32-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86.zip)\n",
    "  - [Windows (64-bit)](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Windows_x86_64.zip)\n",
    "+ Place the Unity Environment zip file in the `p1_navigation/` folder of the repository cloned in the first step, and unzip the file.\n",
    "+ Clone this repository into the `p1_navigation/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5c8e",
   "metadata": {},
   "source": [
    "### Supplemental Packages\n",
    "Run the following code cell *once* to install additional packages required by the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063baf0",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Run the following code cell at every kernel instance start-up to bring implementation dependencies into the notebook namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051dce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6bfa1",
   "metadata": {},
   "source": [
    "## Description of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3555",
   "metadata": {},
   "source": [
    "Deep Reinforcement Q-Learning is a *value-based* class of reinforcement learning algorithms.  These algorithms aim to accurately approximate either the expected reward or reward probability distribution, for every possible pair (state, agent response) in the environment.  With either of these approximations, an agent may be controlled by, when in each state, selecting the action with the highest expected reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ea138a",
   "metadata": {},
   "source": [
    "### Value Distribution\n",
    "This implementation, like in aims to find the reward probability distribution [1]:<br><br>\n",
    "$$d_t^{(n)}\\equiv(R_t^{(n)}+\\gamma_t^{(n)}\\textbf{z},\\textbf{p}(S_{t+n},a^{*}_{t+n}))$$\n",
    "<br>\n",
    "This is an *n-step* value distribution.  The value of the random variable $d_t^{(n)}$ is the sum $R_t^{(n)}$ of the rewards over the next *n* environment time steps, plus the reward distribution $\\textbf{z}$ discounted by the factor $\\gamma_t^{(n)}$.  The probabilities for the values for the random variable are those that result from, when the agent is in the state $S_{t+n}$, *n* steps advanced from present, the optimal action $a^{*}_{t+n}$ is selected. <br><br>\n",
    "In practice, the continuous distribution of values is approximated by histogram binning.  The bins are called *atoms* in the literature and typically form an evenly spaced grid between maximum and minimum allowed values $v_{max}$ and $v_{min}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85920c",
   "metadata": {},
   "source": [
    "### Parameterized Model\n",
    "As the product of the state and action spaces is very large (infinite, since the state variables are continuous), it is necessary to represent the reward distribution with a parameterized function.  The 'Deep' in Deep Reinforcement Q-Learning implies that the parameterized function is going to be a multi-layer neural network.\n",
    "\n",
    "Using the notation in [1], let $p_{\\theta}^i(s,a)$ denote this function, with set of parameters $\\theta$.  Optimization of the parameters in $\\theta$ shall be performed, such that, given the selection of an action $a$ by the agent, when the environment is in state $S$, $p_{\\theta}^i(s,a)$ approximates the probability that the *n-step* reward will be $z_i$.  As in [1], the available $z_i$ will be defined by:<BR><BR>\n",
    "$$z_i \\equiv v_{min} + (i-1)\\frac{v_{max}-v_{min}}{N_{atoms}-1},  i \\in {1,...,N_{atoms}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a47f4",
   "metadata": {},
   "source": [
    "## Implementation of Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8227f",
   "metadata": {},
   "source": [
    "### Hyperparameter Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019fa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f15749b",
   "metadata": {},
   "source": [
    "### Multi-step Learning, Prioritized Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a80a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6aaccc4",
   "metadata": {},
   "source": [
    "### Action Value Distribution Function (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b2af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb36e9e8",
   "metadata": {},
   "source": [
    "### Bellman Update (Loss) Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cde12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50ad606",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276a68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e1126b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b341eb",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Hessel et. al., Rainbow: Combining Improvements in Deep Reinforcement Learning, arXiv:1710.02298 <br>\n",
    "[2] Bellemare et. al., A Distributional Perspective on Reinforcement Learning, arXiv:1707.06887 <br>\n",
    "[3] Schaul et. al., Prioritized Experience Replay, arXiv:1511.05952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
