# BananaNavCode Documentation

Note: This documentation was automatically generated by GPT-4, 25May2023

# SumTreeNode Class

## Overview

The SumTreeNode class provides a representation of a node in a SumTree data structure. This tree-based data structure is commonly used for prioritized experience replay in reinforcement learning (RL), where experiences are stored with certain priorities (probabilities of being sampled). In the SumTree, the probability of a leaf node being chosen is equal to its priority divided by the total of all priorities. This allows for more efficient prioritized sampling of experiences for training the RL agent.

## Attributes

Each SumTreeNode object has the following attributes:

* `data`: Represents the data associated with a node, typically an experience tuple.
* `p_i`: The priority of the node.
* `parent`: Reference to the parent node.
* `left_child`: Reference to the left child of the node.
* `right_child`: Reference to the right child of the node.

## Methods

The class has the following methods:

### `__init__(self, data=None, p_i=0)`

Initializes the SumTreeNode object.

### `update_p(self, delta_p)`

Updates the priority of the node by adding `delta_p` to the current priority. If the node has a parent, the same operation is performed on it recursively. This helps in maintaining the integrity of the sum tree.

### `attach_child(self, child)`

Attaches a new child node to the current node.

### `remove(self)`

Removes the node from the tree and adjusts the tree accordingly to maintain its integrity.

### `weighted_retrieve(self, p_samp)`

Returns the leaf node that is to be selected based on the sample priority `p_samp`.

### `minimal_node(self)`

Returns the node with the lowest sampling probability. This method is useful when looking for nodes to remove from the tree.

### `tree_str(self, lvl=0)`

Provides a string representation of the tree.

## Usage

To use the `SumTreeNode` class, you would typically instantiate an object of the class, providing it with data and a priority. Subsequently, you can attach child nodes using the `attach_child` method, update the priority using the `update_p` method, and remove nodes with the `remove` method. For sampling experiences, you would use the `weighted_retrieve` method.

# PrioritizedReplayBuffer Class

## Overview

The `PrioritizedReplayBuffer` class provides an implementation of a buffer for prioritized experience replay, a key concept in reinforcement learning (RL) where experiences are stored with certain priorities that affect their chances of being sampled for training an RL agent. 

The `PrioritizedReplayBuffer` class uses an instance of the `SumTreeNode` class as the underlying data structure to store the experiences. 

## Attributes

Each `PrioritizedReplayBuffer` object has the following attributes:

* `buffer_life`: Represents the lifespan of the replay buffer. After a certain number of samples, the buffer is flushed and reset.
* `omega`: The exponent that affects the priority calculation for new experiences.
* `store`: An instance of the `SumTreeNode` class, representing the experience store.
* `sample_count`: The number of samples taken from the replay buffer so far.
* `exp_count`: The number of experiences added to the buffer.
* `beta`: A parameter influencing the calculation of importance sampling weights.

## Methods

The class has the following methods:

### `__init__(self, replay_buffer_hyperparameters=ReplayBufferHyperparameters())`

Initializes the `PrioritizedReplayBuffer` object.

### `__len__(self)`

Returns the current number of experiences in the buffer.

### `add_experience(self, experience, loss)`

Adds a new experience to the buffer, computing its priority based on the given `loss` and the `omega` attribute.

### `sample(self, batch_size)`

Returns a batch of experiences sampled according to their priorities, along with meta-information for importance sampling. It also checks if the `buffer_life` has been reached, in which case the buffer is flushed and reset.

## Usage

To use the `PrioritizedReplayBuffer` class, you would typically instantiate an object of the class, providing it with the appropriate hyperparameters. Subsequently, you can add experiences using the `add_experience` method and sample experiences with the `sample` method. The number of experiences in the buffer can be obtained using the `__len__` method.

# NoisyLinear Class

## Overview

The `NoisyLinear` class provides an implementation of a NoisyNet linear layer, which is a fully-connected layer that introduces random noise into its weights. This approach is a common technique used in reinforcement learning (RL) to aid exploration.

This class inherits from PyTorch's `nn.Module`, meaning it can be used as part of a PyTorch neural network model.

## Attributes

Each `NoisyLinear` object has the following attributes:

* `deterministic_linear`: An instance of PyTorch's `nn.Linear` module, representing the deterministic part of the linear transformation.
* `noisy_weights`: The noise to be added to the weights.
* `noisy_bias`: The noise to be added to the bias.

## Methods

The class has the following methods:

### `__init__(self, in_dim, out_dim, noise_init=0.05)`

Initializes the `NoisyLinear` object.

### `noise_transform(self, x)`

Applies a transformation to the input tensor `x` that models factorized Gaussian noise. This transformation is based on Section 3(b) of the paper referenced in the comment.

### `forward(self, x)`

Performs the forward pass of the noisy linear layer, adding noise to the weights and the bias. 

## Usage

To use the `NoisyLinear` class, you would typically instantiate an object of the class, providing it with the input and output dimensions. This class can then be used as part of a PyTorch model. 

For example, the forward pass of your model could include a line like:

```python
x = self.noisy_linear_layer(x)
```

where `self.noisy_linear_layer` is an instance of the `NoisyLinear` class.

Please note that this class expects input tensors to be of the appropriate shape and on the correct device (CPU or GPU). It's important to ensure that your data is correctly formatted before passing it through this layer.

# ParameterizedModel Class

## Overview

The `ParameterizedModel` class is an implementation of a parametric model designed to predict action-value distribution in a reinforcement learning (RL) context. This class inherits from PyTorch's `nn.Module`, which means it can be used as part of a PyTorch neural network model.

This model is especially useful when dealing with environments with a continuous or large discrete action space. It utilizes `NoisyLinear` layers (not included in the provided code, but assumed to introduce random noise into the linear layers for reinforcement learning exploration) and implements a distributional approach to approximate the return distribution for each action.

## Attributes

Each `ParameterizedModel` object has the following attributes:

* `distribution_hyperparameters`: Hyperparameters specific to the action-value distribution.
* `environment_hyperparameters`: Hyperparameters specific to the environment the RL agent operates in.
* `model_hyperparameters`: Hyperparameters specific to the model itself.
* `num_actions`: The number of possible actions the agent can take in the environment.
* `n_atoms`: The number of atoms representing the distribution in the distributional approach.
* `reg_layers`: A sequential container of the layers in the model.

## Methods

The class has the following methods:

### `__init__(self, environment_hyperparameters, model_hyperparameters, distribution_hyperparameters)`

Initializes the `ParameterizedModel` object, setting up the hyperparameters and building the model layers.

### `forward(self, x)`

Performs the forward pass of the model, returning the action-value distribution for each possible action. 

## Usage

To use the `ParameterizedModel` class, you would typically instantiate an object of the class, providing it with the appropriate hyperparameters. This class can then be used as part of a PyTorch model. 

For example, the forward pass of your model could include a line like:

```python
action_distributions = self.parameterized_model(x)
```

where `self.parameterized_model` is an instance of the `ParameterizedModel` class.

Please note that this class expects input tensors to be of the appropriate shape and on the correct device (CPU or GPU). It's important to ensure that your data is correctly formatted before passing it through this layer.

# Bellman_Update Function

## Overview

The `Bellman_Update` function is a crucial component of reinforcement learning (RL), particularly in distributional RL and prioritized experience replay, where it's used to compute the TD (Temporal Difference) error and update the Q-values in an experience replay buffer.

This function performs the Bellman update on a batch of experiences using a policy network and a target network. The policy network is used for selecting actions, and the target network is used to generate the target Q-values for the Bellman update.

## Parameters

* `policy_net`: The network used for deciding which actions to take.
* `target_net`: The network used for generating the target Q-values in the Bellman update.
* `experiences`: A list of experiences sampled from the experience replay buffer. Each experience includes the state, action, reward, and the next state.
* `device`: The device (cpu or gpu) where the computations will be carried out.

## Returns

* `biased_loss`: The TD-error for each experience, corrected for distribution change using importance sampling weights.
* `w_i`: The weights used for correcting the distribution change due to prioritized sampling.

## Usage

To use the `Bellman_Update` function, you need to pass a batch of experiences along with the policy network and the target network.

For example:

```python
biased_loss, w_i = Bellman_Update(policy_net, target_net, experiences, device)
```

This function will return the corrected TD-error and the importance sampling weights for each experience in the batch.

## Notes

The `Bellman_Update` function uses the KL divergence loss to calculate the TD-error between the target and current Q-value distributions. It then multiplies the loss by importance sampling weights to correct for the distribution change due to prioritized sampling. 

Also, it's important to note that the code heavily relies on PyTorch tensors and its specific operations. Ensure that your environment has PyTorch installed and the inputs to the function are in the appropriate format. 

This function does not perform the backpropagation and update steps itself; you would need to do that separately, after calling this function.

# tensor_check Function

## Overview

The `tensor_check` function is a utility function designed to perform diagnostics on PyTorch tensors. This function checks if any values in a tensor are `NaN` or `Inf`, and if such values are found, it prints out a message with the problematic tensor and its descriptor.

## Parameters

* `input`: The PyTorch tensor to be checked.
* `desc`: A descriptor or label for the tensor, used for identification when printing diagnostic messages.

## Returns

This function does not return any value; its purpose is to print diagnostic messages in case of problematic tensor values (`NaN` or `Inf`).

## Usage

To use the `tensor_check` function, pass the tensor you want to check and its descriptor as arguments. 

For example:

```python
tensor_check(my_tensor, 'My Tensor')
```

If any `NaN` or `Inf` values are present in `my_tensor`, the function will print a message with the descriptor ('My Tensor') and the problematic tensor.

## Notes

The `tensor_check` function can be a useful tool during the debugging process, especially when dealing with complex neural networks where the cause of `NaN` or `Inf` values can be challenging to trace.

It's important to note that this function doesn't modify the tensor or handle the occurrence of `NaN` or `Inf` values. It only alerts you when such values are present. If these problematic values are detected, you should take further steps to diagnose and resolve the issues causing them in your code.

# PerformanceLogger Class

## Overview

The `PerformanceLogger` class provides a mechanism to keep track of episodic scores in a reinforcement learning setting and compute a running average of these scores. This class can be useful for monitoring the performance of an agent during training and evaluation stages.

## Class Initialization

The class is initialized with the following parameters:

* `avg_window_size` (default=100): Size of the window for calculating the running average. 
* `starting_scores` (default=None): If provided, these scores will be used as the initial set of episodic scores.

Example:

```python
logger = PerformanceLogger(avg_window_size=50, starting_scores=[1, 2, 3, 4, 5])
```

## Attributes

* `avg_window_size`: Size of the window for calculating the running average.
* `scores`: List of scores added to the logger.
* `internal_run_avg`: Current running average of the scores.

## Methods

### add_score(score)

This method is used to add a new score to the logger. The score is appended to the `scores` list, and the `internal_run_avg` is updated. If the number of scores exceeds the `avg_window_size`, the oldest score (score at the tail of the window) is removed from the running average.

### run_avg()

This method returns the current running average of scores. If the number of scores is less than `avg_window_size`, the method returns `None`.

## Usage

```python
logger = PerformanceLogger(avg_window_size=50)

# Add a new score
logger.add_score(200)

# Get the running average
average = logger.run_avg()
```

## Note

The running average is a useful metric in reinforcement learning as it provides a smoother estimate of an agent's performance over time, reducing the impact of individual episodic score variability. The `PerformanceLogger` class provides a convenient way to compute and track this running average.

# MultistepBuffer Class

## Overview

The `MultistepBuffer` class is used in reinforcement learning algorithms that consider multiple steps in the environment for a single update, such as n-step SARSA or n-step Q-learning. This class provides a way to store and retrieve these n-step experiences.

## Class Initialization

The class is initialized with the following parameters:

* `n_step_order`: The number of steps considered for each update. For example, for n-step Q-learning, this would be 'n'.
* `gamma` (default=1.0): The discount factor used in reinforcement learning algorithms.

Example:

```python
buffer = MultistepBuffer(n_step_order=3, gamma=0.99)
```

## Attributes

* `n_step_order`: The number of steps considered for each update.
* `store`: A deque data structure used to store the experiences. Its maximum length is set to `n_step_order + 1`.
* `gamma`: The discount factor used in reinforcement learning algorithms.

## Methods

### add_experience(exp)

This method is used to add a new experience to the buffer. The experience is appended to the `store` deque.

### ready()

This method checks whether the buffer has enough experiences to form an n-step experience. It returns True if the buffer's length equals `n_step_order + 1`, and False otherwise.

### get_n_step_experience()

This method returns an n-step experience that combines the state and action from the oldest experience in the buffer, the accumulated reward over n steps, and the final state from the most recent experience in the buffer. The experience is returned as a `SumTreeNode` object.

## Usage

```python
buffer = MultistepBuffer(n_step_order=3, gamma=0.99)

# Add a new experience
buffer.add_experience(exp)

# Check if the buffer is ready
if buffer.ready():
    n_step_experience = buffer.get_n_step_experience()
```

## Note

The `MultistepBuffer` class is specifically designed to work with environments where an agent's actions can affect future states and rewards over multiple steps, not just the immediately next step. By using this class, we can efficiently generate n-step experiences for use in reinforcement learning algorithms.

# sel_action Function

## Overview

The `sel_action` function is used in reinforcement learning algorithms to select an action for a given state, based on a policy network. The function calculates the mean action values by using the policy network and then selects the action with the highest mean value.

## Function Parameters

The function takes the following parameters:

* `policy_net`: The policy network that the function uses to calculate the action values.
* `state`: The state for which the function selects an action.
* `device`: The PyTorch device on which the calculations are performed.

## Function Output

The function returns the action with the highest mean value.

## Usage

```python
action = sel_action(policy_net, state, device)
```

## Function Body

The function starts by extracting the `z_i`, `num_act`, and `n_atoms` values from the policy network's hyperparameters. It then calculates the `d_t` values by passing the state through the policy network.

For each possible action, the function calculates the mean action value by multiplying the corresponding `d_t` values by `z_i` and summing the results. The function then concatenates these mean values into the `mean_action_values` tensor.

Finally, the function selects the action with the highest mean value and returns it.

## Note

This function is specifically designed to work with reinforcement learning algorithms that use a policy network to select actions. The policy network can be any neural network trained to predict the expected rewards for different actions in different states.

# train_net Function

## Overview

The `train_net` function performs the training of the given policy network using an environment and other parameters. It uses the experience replay approach for improving the performance of the reinforcement learning policy network.

## Function Parameters

The function takes the following parameters:

* `policy_net`: The policy network that is trained by the function.
* `env`: The environment in which the policy network is trained.
* `opt_hyp`: An instance of the `OptimizationHyperparameters` class. It defines the hyperparameters for the optimization process. Default value is `OptimizationHyperparameters()`.
* `replay_buffer`: An instance of the `PrioritizedReplayBuffer` class. It provides a buffer for storing and retrieving experiences in the reinforcement learning process. Default value is `PrioritizedReplayBuffer()`.
* `train_options`: An instance of the `TrainingOptions` class. It defines various options for the training process. Default value is `TrainingOptions()`.

## Function Output

The function returns an instance of `PerformanceLogger` which logs the performance of the policy network during the training process.

## Function Body

The function starts by checking the availability of a GPU and sets up the PyTorch device accordingly. It then moves the policy network to this device. For readability, it also creates aliases for several variables.

It sets up an optimizer (Adam) for the policy network with certain parameters and generates a target network, initializing its parameters to zero to avoid initial noise.

The function then goes through a loop for a maximum number of episodes, defined in the training options. In each episode, the function interacts with the environment, processes the interaction results, updates the replay buffer, and possibly performs learning and optimization steps.

At the end of each episode, the function updates the performance log and checks whether the policy network has achieved the desired performance level (solved the problem). If the network has not solved the problem by the end of the maximum number of episodes, the function indicates this with a message.

## Usage

```python
perf_log = train_net(policy_net, env, opt_hyp, replay_buffer, train_options)
```

## Note

This function is designed to work with reinforcement learning algorithms that use a policy network and experience replay. The environment, optimization hyperparameters, replay buffer, and training options are all designed to be customizable to suit different reinforcement learning problems and configurations.
